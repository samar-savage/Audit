{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 2.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx) (4.6.1)\n",
      "Building wheels for collected packages: python-docx\n",
      "  Building wheel for python-docx (setup.py): started\n",
      "  Building wheel for python-docx (setup.py): finished with status 'done'\n",
      "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184605 sha256=5ecd31dc936eb03703f1b1234b7bbe9c5aea71cb50b168e05d3a35248dccaef9\n",
      "  Stored in directory: c:\\users\\samar\\appdata\\local\\pip\\cache\\wheels\\32\\b8\\b2\\c4c2b95765e615fe139b0b17b5ea7c0e1b6519b0a9ec8fb34d\n",
      "Successfully built python-docx"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-0.8.11\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Report_Samar_Jberi.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "docx.document.Document"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document.paragraphs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.paragraphs[270].text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " paragraph 1 is\n",
      "In the case of IT product advertisement, there are a few crucial channels of reaching end-users:\n",
      "\n",
      " paragraph 3 is\n",
      "Articles on the blog\n",
      "\n",
      " paragraph 4 is\n",
      "Guest and sponsored articles\n",
      "\n",
      " paragraph 5 is\n",
      "Mailing (including newsletter)\n",
      "\n",
      " paragraph 6 is\n",
      "Video channels (e.g. Youtube, Vimeo)\n",
      "\n",
      " paragraph 7 is\n",
      "Social media and forums (e.g. Facebook Groups, Quora, Reddit)\n",
      "\n",
      " paragraph 9 is\n",
      "It’s extremely unlikely that the IT product you’ve created, especially at its early stage, won’t need any advertising to gain popularity. But even if yes, users are the key to spread the information about it. Reaching new customers should give-us exponential growth.\n",
      "\n",
      " paragraph 10 is\n",
      "During the pandemic, virtual events still serve as an effective promotion tool.\n",
      "\n",
      " paragraph 12 is\n",
      "Host a Facebook Live session demoing the product and highlighting its features.\n",
      "\n",
      " paragraph 13 is\n",
      "Run a live or recorded Q&A on the product itself.\n",
      "\n",
      " paragraph 15 is\n",
      "Call to action\n",
      "\n",
      " paragraph 17 is\n",
      "By now, business decision is highly impacted by business tools used.\n",
      "\n",
      " paragraph 18 is\n",
      "Business intelligence is one of these important tools. As per as revolution 4.0 companies have to put in place unintuitive tools that allow decision maker to take the right decision at the right time reason why we do suggest our business solution to the insurance compagnies.\n",
      "\n",
      " paragraph 19 is\n",
      "Our solution offers a dashboard that contains \n",
      "\n",
      " paragraph 20 is\n",
      "Number of damaged vehicles by vehicle brand (Customize insurance packages for policyholders with cars depending on their brands.)\n",
      "\n",
      " paragraph 21 is\n",
      "Number of claims by region (Customize insurance packages for policyholders traveling in disaster areas.)\n",
      "\n",
      " paragraph 22 is\n",
      "Ratio of road accidents by percentage of liability (Profile the most affected drivers: making responsible claims with normal insurance contracts).\n",
      "\n",
      " paragraph 23 is\n",
      "Number of claims by type of claim (Customize packages and invest more in life insurance.)\n",
      "\n",
      " paragraph 24 is\n",
      "Number of road accidents by age (Customize insurance packages for policyholders of risky ages.)\n",
      "\n",
      " paragraph 25 is\n",
      "Guaranteed and profitable !\n"
     ]
    }
   ],
   "source": [
    "index=0\n",
    "for para in document.paragraphs:\n",
    "    index+=1\n",
    "    if(len(para.text)>0):\n",
    "            print(\"\\n paragraph\",index,\"is\")\n",
    "            print(para.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docx_to_dict(name_of_file):\n",
    "    docx_dict={}\n",
    "    document = Document(name_of_file)\n",
    "    indx=0\n",
    "    for para in document.paragraphs:\n",
    "        indx+=1\n",
    "        docx_dict[indx]= para.text\n",
    "    return docx_dict\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\english.docx\n",
      "C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\Rapport.docx\n",
      "C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\Rapport_text_mining.docx\n",
      "C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\~$apport.docx\n"
     ]
    }
   ],
   "source": [
    "directory='C:/Users/Samar/Desktop/5BI4/Stage PFE/essay'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\") or filename.endswith(\".doc\") and not filename.startswith('~$'):\n",
    "        print(os.path.join(directory,filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\\\Rapport.docx'])\n",
      "dict_keys(['C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\\\Rapport.docx', 'C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\\\Rapport_text_mining.docx'])\n"
     ]
    }
   ],
   "source": [
    "directory='C:/Users/Samar/Desktop/5BI4/Stage PFE/essay'\n",
    "docx_content={}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".docx\") or filename.endswith(\".doc\"):\n",
    "        path_to_docx = os.path.join(directory,filename)\n",
    "        docx_content[path_to_docx ] = docx_to_dict(path_to_docx)\n",
    "        print(docx_content.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\\\Rapport.docx', {1: '', 2: '', 3: '', 4: '  ', 5: \"Ecole Supérieure Privée d'Ingénierie\", 6: 'Et de Technologies', 7: '', 8: 'Engineer Summer internship', 9: '', 10: '', 11: '', 12: '', 13: '', 14: '', 15: '', 16: '', 17: 'Developed by Samar Jberi', 18: 'Supervised by Mohammed Tlili', 19: 'August 6th to September 30th, 2021', 20: '', 21: 'Academic Year: 2021/2022', 22: '', 23: '', 24: '', 25: '', 26: '', 27: '', 28: '', 29: 'Summary', 30: 'General Introduction……………………………………………………………………………………………………………………………….5', 31: 'I           General project context………………………………………..………………………………………………………………………6', 32: '1.\\tPresentation of the host organization……………………………..………………………………………………………….6', 33: '2.\\t Project context……………………………………………………………………………..……………………………………..…….6', 34: '2.1.\\t Study of the existing…………………..…………………………………………………….……………….………….…….....…6 ', 35: '2.2.\\tProblematic………………………………………………………………………………………………….……………...…………....7', 36: '2.3.\\tProposed solutions………………...…………………………………………………….……………………………………...……7', 37: '2.4.\\tAdopted methodology - CRISP-DM……..…………………………………………….…………………………….………….8', 38: '3\\tImplementation……………………………………………………………………………………….……………………………...…9', 39: '3.1.\\tUsed tools …………………………………………………………………………………………………….…………………………...9', 40: '3.1.1.\\tMicrosoft Power BI…………………………………………………………………………………….……………………………….9', 41: '3.1.2.\\tPython…………………………………………………………………………………………………………………………………….....9', 42: '4.           Data Warehouse ……………………………………………………………………….……………………………………………….9', 43: '4.1.        Data Warehouse modeling approach..............................................................................................9 ', 44: 'II.             Data Scraping...............................................................................................................................11', 45: '1.         Data integration using Power BI………………………………………………………………………………………………....12', 46: '1.2.        Data warehouse model.................................................................................................................17', 47: 'III.            Text Mining……………………………………………………………………………………………………………………………..20', 48: 'IV.\\tData visualisation …..………………………………………………………………………………………………………………..27', 49: 'V.\\tConclusion……………….……………………………………………………………………………………………………………....30', 50: '', 51: '', 52: '', 53: 'Table of figures', 54: '', 55: 'Figure 1: Esprit Logo……………………………………………………………………………………….6', 56: 'Figure 2: CRISP-DM Methodology………………………………………………………………………...8', 57: 'Figure 3: Bottom-Up  approach by Ralph Kimball………………………………………………………..10', 58: 'Figure 4: Connecting to the web Page…………………………………………………………………….11', 59: 'Figure 5: Data picking and storing in csv file……………………………………………………………..12', 60: 'Figure 6: Calendar Table  ………………………………………………………………………………....13                                                             ', 61: 'Figure 7: Posts_Fact_table..………………………………………………………………………….……14', 62: 'Figure 8: Revieweres LookUp…………………………………………………………………………….15', 63: 'Figure 9: Skills LookUp_Table……………………………………………………………………………16', 64: 'Figure 10: What They_Do LookUp table…………………………………………………………………16', 65: 'Figure 11: Where_They_live lookUp Table……………………………………………………………....17', 66: 'Figure 12: Where_They_Studied LookUp Table………………………………………………………….17', 67: 'Figure 13: LinkedIn Warehouse…………………………………………………………………………..18', 68: 'Figure 14: Facebook Warehouse ………………………………………………………………………….19                                                     ', 69: 'Figure15: Cleaning data …………………………………………………………………………………..20 ', 70: 'Figure 16: Study of frequency…….………………………………………………………………………21                                                ', 71: 'Figure 17: Frequency plot of single terms……………………………………………………………....21', 72: 'Figure 18: Content filtering………………………………………………………………………………..22', 73: 'Figure 19: Frequency Plot of coupled term……………………………………………………………….22', 74: 'Figure 20: Word Lemmatisation ……………………………………………………………………….....23                    ', 75: 'Figure 21: Words_Clouds ………………………………………………………………………………...23      ', 76: 'Figure 22: Bar_Plot………………………………………………………………………………………..24                                                           ', 77: 'Figure 23: Number of Likes basing on gender……………………………………………………………25', 78: 'Figure 24: General aspect………………………………………………………………………………....25', 79: 'Figure 25: Data_training…………………………………………………………………………………..26', 80: 'Figure 26: Facebook_dashboard_1………………………………………………………………………..27', 81: 'Figure27: facebook_Dashboard_2………………………………………………………………………...27', 82: 'Figure 28:Skills………………………………...………………………………………………………….28', 83: 'Figure 29:Where_They_Live……………………………………………………………………………...28', 84: 'Figure 30:Where_They_Studied…………………………………………………………………………..29', 85: 'Figure 31:What_They_Do………………………………………………………………………………...29', 86: '', 87: '', 88: '', 89: '', 90: '', 91: '', 92: '', 93: '', 94: '', 95: '', 96: '', 97: '', 98: '', 99: '', 100: '', 101: '', 102: '', 103: '', 104: '', 105: '', 106: '', 107: '', 108: '', 109: '', 110: '', 111: '', 112: 'General Introduction', 113: 'In order to apply the different skills we have aquired during the past couple of years, students conduct a summer internship after their second year of engineering studies.', 114: 'Supported by the Private Higher School of Engineering and Technologies Esprit, I was offered the opportunity to work on the subject entitled \"Social Network analysis\" that took place from 06/08/2020 to 30/09/2021, under the supervision and help of Mr Mohammed Tlili.', 115: 'During this internship I had the chance to discover new technologies and acquire new skills.', 116: 'Dived into the data and created a neat deliverable out of it.', 117: 'In this report, I will list the different tasks performed as well as the steps I followed to develop my project.', 118: 'In the first chapter, I will start by a presentation of the subject, study of the existing, and proposed solutions.', 119: 'The second chapter will deal with the text mining part of my project, while the third chapter will deal with data visualization. Finally a conclusion will summarize the work done and the results obtained.', 120: '', 121: '', 122: '', 123: '', 124: '', 125: '', 126: '', 127: '', 128: '', 129: '', 130: '', 131: '', 132: '', 133: '', 134: '', 135: 'I. General project context:', 136: 'In this chapter I will start by presenting the host organization, followed by a deep analysis of the project context including a study of the existing, the problematic at hand and the proposed solution. Lastly, I will specify the adopted methodology.', 137: 'Presentation of the host organization:', 138: '', 139: '', 140: '', 141: '', 142: '', 143: 'Founded in 2003, Esprit has managed to become an internationally recognized establishment.', 144: 'In addition to being accredited by the Tunisian Ministry of Higher Education and Scientific Research, the quality of Esprit education has become well renown over the years.', 145: 'Following the lead of bigger higher education facilities such as MIT, Esprit builds the pedagogy of engineering training around professional scenarios. Esprit aims towards excellence.', 146: 'And for this exact reason Esprit cares deeply about its public image, collecting and taking into consideration all the feedback in order to continuously improve its programs and keep up with the expectations to offer the best college experience possible for its students and employees.', 147: '2 Project context:', 148: '1 .Study of the existing:', 149: \"Esprit is an engineering school, that's all, and it trains students according to the most demanding international quality standards. Esprit brings together a very broad number of skills in the sector: more than 60 experts, including the main aspects in public engineering studies reforms, around 100 permanent teachers. All in the service of a unique but very essential objective: to train excellent engineers ready to take over the IT market.\", 150: 'From around 30 in 2003 to over 2,400 in 2010-2011 (day and evening schooling), the number of students has not ceased to increase, with an average annual growth rate of over 85%.', 151: '', 152: '2.\\t2.Problematic:', 153: 'According to a 2014 British Council report titled “Education in North Africa”, since then the system has developed to include 198 public higher education institutions, 63 private institutions, 24 higher institutes of technical studies and six higher institutes for teacher training. The expansion of the higher education system led a boom from 17,000 students in 1975 to half a million students in 2015(equivalent to 8% of the country’s student population), which represents an increase in the enrolment rate at the tertiary level from 2.6% in 1974 to 35.2% by 2015.', 154: 'With such tight competition within the private sector, each private institute aims to step up in order to maintain its success and notoriety.', 155: 'I don’t see any problem here: you should formulate a question that your project answers', 156: '2.3. Proposed solutions:', 157: 'Based on identified needs and my knowledge about the subject since I’m an Esprit student myself, I was assigned the project “Social Network analysis Esprit” in pursuance of finding solutions for the issues mentioned in the problematic.', 158: 'Since every private facility relies heavily on its public image and, we realized that it is an essential starting point to consult the public reviews online about Esprit , study its foundation and followers and evaluate the interactions that take place on the social networks in order to identify and solve the most frequent problems.', 159: 'To serve this purpose, I had to go through several steps:', 160: 'Scrapping the most relevant information (followers, employees, comments,interactions…) about Esprit on social networks (Facebook , LinkedIn)', 161: 'Shaping and cleaning the data, establishing relationships between the different data.', 162: 'Text Mining to detect the nature of the content and the most frequent searched terms.', 163: 'Using Data Visualization tools to classify posts based on different criteria to make the data more legible and have a general grasp on the image of Esprit. ', 164: '', 165: '', 166: '', 167: '2. 4.Adopted methodology - CRISP-DM ', 168: 'CRISP-DM stands for cross-industry process for data mining. The CRISP-DM methodology provides a structured approach to planning a data mining project. It is a robust and well-proven methodology. CRISP-DM is the most widely used chain for the development of Data Mining projects. It is composed essentially of 6 Stages:', 169: '1. \\t', 170: '2. \\t', 171: '3. \\t', 172: '4. \\t', 173: '5. \\t', 174: '                        6.  ', 175: '', 176: '', 177: '', 178: '', 179: '', 180: '', 181: '', 182: '', 183: '', 184: '', 185: '', 186: '', 187: '', 188: ' Conclusion', 189: 'In this chapter, I got to explain the project’s context starting from the study of the existing to the proposed solution. Then proceeded to present my methodology. In the next chapter, I will be demonstrating the implementation of my data.', 190: '', 191: 'Implemantation:', 192: 'In this chapter I’m going to give a detailed description of the practical work I did. I’m going to start by listing the different tools I used, followed by the data model, screenshots of the integration work on power BI, screenshots of the Text Mining and Data Visualization work.', 193: '3.1.\\tUsed tools ', 194: ' 3.1.1.Microsoft Power BI', 195: 'Power BI is a business analytics service by Microsoft. It aims to provide interactive\\xa0\\xa0and\\xa0\\xa0capabilities with an interface simple enough for end users to create their own reports and dashboards.\\xa0It is part of the\\xa0. Power BI provides\\xa0-based business intelligence services, known as \"Power BI Services\", along with a desktop-based interface, called \"Power BI Desktop\". It offers\\xa0\\xa0capabilities including\\xa0,\\xa0\\xa0and interactive dashboards.', 196: '3.1.2.Python ', 197: 'Python\\xa0is an\\xa0\\xa0\\xa0. Python\\'s design philosophy emphasizes\\xa0\\xa0with its notable use of\\xa0. Its\\xa0\\xa0as well as its\\xa0\\xa0approach aim to help\\xa0\\xa0write clear, logical code for small and large-scale projects. Python is\\xa0\\xa0and\\xa0. It supports multiple programming paradigms, including\\xa0\\xa0(particularly,\\xa0),\\xa0\\xa0and\\xa0. Python is often described as a \"batteries included\" language due to its comprehensive\\xa0.', 198: '4.Data Warehouse ', 199: '4.1.Data Warehouse modeling approach ', 200: 'Designing a\\xa0\\xa0is an essential part of business development. In this segment, I’m going to take an in depth look at the data warehouse modeling approach that I chose:', 201: ' Bottom-Up approach by Ralph Kimball: ', 202: 'Contrast\\xa0to Bill Inmon approach, Ralph Kimball recommends building the data warehouse that follows the bottom-up approach. In Kimball’s philosophy, it first starts with mission-critical data marts that serve analytic needs of departments. Then it is integrating these data marts for data consistency through a so-called information bus. Kimball makes uses of the dimensional model to address the needs of departments in various areas within the enterprise.', 203: '', 204: '', 205: '', 206: '', 207: '', 208: '', 209: '', 210: '', 211: '', 212: '', 213: '', 214: 'For my data model approach, I chose the Bottom-Up approach by Ralph Kimball. My project’s specific objectives required me to define the data Marts and then merge them together. Moreover, our data model takes the shape of the Star schema, which is used for the Bottom-Up approach. ', 215: '', 216: '', 217: 'II.\\tData Scrapping:', 218: 'In order to gather maximum of information about Esprit online, I did some web scraping using python, relying essentially on two important libraries: selenium and Beautifulsoup.', 219: 'Beautiful Soup is a Python HTML and XML document parsing library created by Leonard Richardson. It produces a syntax tree which can be used to search for elements or modify them.', 220: '', 221: 'Selenium is a powerful tool for controlling web browsers through programs and performing browser automation. It is functional for all browsers, works on all major OS and its scripts are written in various languages i.e.\\xa0,\\xa0,\\xa0.', 222: 'l will be working with Python.', 223: '', 224: '', 225: '', 226: '', 227: '', 228: '', 229: '1.Data integration using power BI', 230: 'Data integration is the process of combining data from different sources into a single, unified view. Integration begins with the ingestion process and includes steps such as cleansing and transformation. Data integration ultimately enables analytics tools to produce effective, actionable business intelligence.', 231: 'Data integration architects develop data integration software programs and data integration platforms that facilitate an automated data integration process for connecting and routing data from source systems to target systems. An example of those software is Power BI.', 232: 'In the following screenshots, I will demonstrate my work in how to clean the data (the data scrapped earlier) and loading it into my data warehouse. ', 233: '', 234: '', 235: '', 236: ' ', 237: 'This figure demonstrates a detailed calendar of the posts updates which make it very flexible and useful with the data visualization. ', 238: '', 239: '', 240: ' ', 241: '', 242: '', 243: 'This figure indicates posts informations, starting with the comments, number of likes, number of reacting comments, type of comment (wether postive or negative), along with the foreign keys of the lookUp tables in which our Data table is associated.', 244: '', 245: '', 246: '', 247: '', 248: '', 249: '', 250: '', 251: 'This figure shows the information about the Reviewers members, starting by their full names, recommendation (whether they recommend esprit or not), their gender and their nationality.', 252: '', 253: '', 254: '', 255: '', 256: '', 257: '', 258: '', 259: ' ', 260: '', 261: ' ', 262: 'This figure demonstrates the number of esprit employees matched to their skill according to LinkedIn.', 263: '', 264: '', 265: '', 266: '', 267: '', 268: '', 269: '', 270: '', 271: '', 272: '', 273: '', 274: '', 275: '', 276: 'This figure demonstrates the number of Esprit employees matched to their occupation according to LinkedIn.', 277: '', 278: '', 279: '', 280: '', 281: '', 282: '', 283: '', 284: '', 285: '', 286: '', 287: '', 288: '', 289: '', 290: 'This figure demonstrates the number of Esprit employees matched  to where they live  according to LinkedIn.', 291: '', 292: '', 293: '', 294: '', 295: '', 296: '', 297: '', 298: '', 299: '', 300: '', 301: '', 302: '', 303: 'This figure demonstrates the number of Esprit employees matched to where they studied according to LinkedIn.', 304: '2. Data warehouse model:', 305: 'Since Data was gathered from two completely different sources (LinkedIn and Facebook) I had to create two different Data warehouses.', 306: '', 307: '', 308: '', 309: '', 310: '', 311: '', 312: '', 313: '', 314: '', 315: '', 316: '', 317: '', 318: '', 319: '', 320: '', 321: '', 322: '', 323: '', 324: 'III.Text Mining', 325: 'Text mining, also referred to as\\xa0text data mining, similar to\\xa0text analysis, is the process of deriving high-quality\\xa0\\xa0from\\xa0.\\xa0', 326: 'Within the following, I’ll be demonstrating screenshots about my work of text Mining of esprit reviewers Facebook page.', 327: 'At this first stage, I strated by fetching the Data by scrapping them out of the reviewers’ page on Facebook.', 328: 'After fetching my data, I got down to the text Mining phase. Starting by implementing the necessary liberaries, cleaning the Data out of stop words and punctuations, applying the different methods such as stemming and Lemmatization. And studiying words frequency and highlighting them in order to have a clear vision about the most significant terms related to Esprit.', 329: '', 330: '', 331: '', 332: 'Figure 16: Studying of frequency', 333: '', 334: '', 335: '', 336: '', 337: '', 338: '', 339: '', 340: '', 341: '', 342: '', 343: '', 344: '', 345: '', 346: '', 347: '', 348: '', 349: '\\t', 350: '', 351: '', 352: '', 353: '', 354: '', 355: '', 356: '', 357: '', 358: '', 359: '', 360: '', 361: '', 362: '', 363: '', 364: 'In this phase, I’m going to take a close look on the mass of data I gathered.', 365: '', 366: '', 367: '', 368: '', 369: ' ', 370: '', 371: '', 372: '', 373: 'Here I will be closuring my sentimental analysis by a supervised learning approach of classification called Naive Bayes to figure out the accuracy of my data since it is popular and used for sentiment classification.', 374: '', 375: '', 376: '', 377: '', 378: '', 379: '', 380: '', 381: '', 382: '', 383: '', 384: '', 385: '', 386: 'IV.Data Visualization with Power BI', 387: 'On this part, I will be demonstrating my final dashboard including the different reports for a neat and clear readability of my data and the relation between its different elements.', 388: '', 389: '', 390: '', 391: '', 392: '', 393: '', 394: '', 395: '', 396: '', 397: 'V.conclusion\\xa0:', 398: 'Since data is massively growing over the years, learning the adequate tools to handle and manage it in the best ways is a shortcut towards successful entrepreneurial planning.', 399: 'During this project, I have been through lots of phases from data scraping to text Mining going to data visualization. Power Bi and Python are two powerful tools that led into creating a dashboard so user friendly, so readable for the optimum results and the best decision making.', 400: ''}), ('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\\\Rapport_text_mining.docx', {1: '', 2: '5 ERP-BI 4', 3: '', 4: '', 5: '                  ', 6: \"Ecole Supérieure Privée d'Ingénierie\", 7: 'Et de Technologies', 8: '', 9: 'Rapport Advanced Learning For Text & Graph Data', 10: '', 11: '', 12: '', 13: 'Ce travail est réalisé par :', 14: 'CHIKHAOUI Nouha', 15: 'HADDAD Mohamed Aziz', 16: 'HATIRA Asma', 17: 'JBERI Samar', 18: 'KOOLI Youssef', 19: 'ZALILA Mahdi', 20: '', 21: '                                    Année académique : 2021/2022', 22: '', 23: 'Sommaire', 24: 'Introduction……………………………………………………………………………………………………………………………………….3', 25: 'I.Contexte du sujet……………..………………………………………………………………………………………………………………4', 26: \"1. Étude de l'existant ...................................... ........................................................................ ........... ...4\", 27: '2. Problématique ..................................................... .............................. ............................ ...................4 ', 28: '3. Solutions proposées..................................... .............................. . .............................. ............... .......4 ', 29: '4.Méthodologie adoptée - CRISP-DM ………....................................... ...................... ..............................5 ', 30: '5.Outils utilisés .............................................. .............................. .............................. ...........................6', 31: '5.1. Python………………………………………………………………………………………………………………………………………….6', 32: '5.2. Kaggle………………………………………………………………………………………………………………………………………….6', 33: 'II.Objectives……………………………………………..…………………………………………………………………………………………7', 34: '1.Text mining………………………………………………………………………………………………………………………………………7', 35: '1.Objective1: Text Mining…………………………………………………………………………………………………………………..7', 36: '2.Objective2: Web Mining………………………………………………………………………………………………………………..18', 37: '3.Objective3: Analyse des sentiments ………………………………………………………………………………………………28', 38: 'III.Conclusion…………………………………………………………………………………………………………………………………….31', 39: '', 40: '', 41: '', 42: '', 43: '', 44: '', 45: '', 46: '', 47: '', 48: '', 49: '', 50: '', 51: '', 52: '', 53: '', 54: 'Introduction', 55: \"Afin d'appliquer les différentes compétences que nous avons acquis au cours des derniers mois, nous sommes comme étudiants en troisième année d'ingénierie censés de réaliser un projet sur l'extraction de texte en commençant par définir trois objectifs et en travaillant jusqu'au bout.\", 56: \"Au cours de ce projet, nous avons eu la chance de découvrir de nouvelles technologies et d'acquérir de nouvelles compétences. Nous avons plongé dans les données et en avons tiré un livrable soigné.\", 57: 'Dans ce rapport, nous allons énumérer les différentes tâches effectuées ainsi que les étapes que nous avons suivies pour développer notre projet.', 58: \"Dans le premier chapitre, nous commencerons par une présentation du sujet, l'étude de l'existant, et les solutions proposées.\", 59: 'Le deuxième chapitre traitera des méthodes utilisées, tandis que le troisième chapitre traitera des objectifs et de leur démonstration. ', 60: 'Enfin, une conclusion résumera le travail effectué et les résultats obtenus.', 61: '', 62: '', 63: '', 64: '', 65: '', 66: '', 67: '', 68: '', 69: '', 70: '', 71: '', 72: '', 73: '', 74: '', 75: '', 76: '', 77: '', 78: 'I.Contexte du sujet', 79: \"Dans ce chapitre, nous commencerons par présenter l'organisme d'accueil, suivi d'une analyse approfondie du contexte du projet comprenant une étude de l'existant, de la problématique à traiter et de la solution proposée.\", 80: '', 81: '1. Étude de l’existant :', 82: \"Les crypto-monnaies existent depuis plus de 10 ans. Tant que beaucoup de gens n'en ont pas une connaissance approfondie, d'autres, quels que soient commerçant ou entrepreneurs dans les secteurs liés à la blockchain, y voient une opportunité.  Facebook, Twitter et Reddit sont devenus des vecteurs très populaires de connaissances sur les crypto-monnaies.\", 83: \"Certains des meilleurs contenus proviennent de grands acteurs, comme la société de blockchain Ripple, qui s'est attachée à partager des contenus éducatifs et des webinaires sur les crypto-monnaies avec ses près d'un million de followers sur Twitter.\", 84: '', 85: '2.  Problématique :', 86: \"Avec une telle quantité de suiveurs et le manque de connaissances des gens sur la culture des crypto-monnaies, il est facile de créer l’espace pour une mauvaise influence puisque les gens ont tendance à former leurs convictions à propos un certain sujet en se basant sur les opinions des gens sur les réseaux sociaux. Il est donc important de suivre l'opinion publique, surtout si la crypto-monnaie est en baisse.\", 87: \"La question est la suivante : si nous recueillons ces opinions, quel type d'information pouvons-nous en tirer ?\", 88: '', 89: '3.  Solutions proposés\\xa0:', 90: 'Le sentiment analysis s\\'agit d\\'un outil marketing puissant qui permet aux gestionnaires de bitcoin de comprendre les émotions des clients. C\\'est un facteur important lorsqu\\'il s\\'agit de la reconnaissance des bitcoins, de la fidélité des clients, de leur satisfaction, du succès de la publicité et de la promotion, et de l\\'acceptation de produit. Comprendre la psychologie des consommateurs peut aider les gestionnaires de bitcoins à modifier leur feuille de route avec plus de précision. Le terme de marketing basé sur les émotions est une expression générale qui englobe les réponses émotionnelles des clients, telles que \"positif\", \"négatif\", \"neutre\", \"négatif\", \"dégoût\", \"frustration\", etc. Comprendre la psychologie des réponses des clients peut également augmenter la valeur des bitcoins.', 91: \"Afin d'aborder les questions mentionnées dans la problématique, nous avons décidé de passer par trois étapes principals:\", 92: 'le Text Mining : scrapé des tweets , netoyer le contenu et visualiser  les termes dominant', 93: 'le web Mining ', 94: 'prediction des sentiment', 95: ' 4.   Adopted methodology - CRISP-DM:', 96: 'Pour ce projet,on a utilisé la méthodologie CRISP-DM qui signifie \"cross-industry process for data mining\". Elle fournit une approche structurée pour la planification d\\'un projet de data mining. Elle est composée essentiellement de 6 étapes :', 97: '1. \\tCompréhension du domaine métier', 98: '2. \\tCompréhension des données', 99: '3. \\tPréparation des données', 100: '4. \\tModélisation', 101: '5. \\tÉvaluation', 102: '6.           Déploiement', 103: '', 104: '', 105: '', 106: '', 107: '', 108: '5. Outils utilisés:', 109: ' ', 110: '5.1 Python:', 111: \"Python est un langage de programmation interprété de haut niveau à usage général. La philosophie de conception de Python met l'accent sur la lisibilité du code grâce à l'utilisation notable d'une indentation importante. Ses constructions de langage ainsi que son approche orientée objet visent à aider les programmeurs à écrire un code clair et logique pour des projets de petite et grande envergure.\", 112: '5.2 Kaggle :', 113: 'Kaggle est une plateforme web organisant des compétitions en Data science. Sur cette plateforme, les entreprises proposent des problèmes liés au Data science et offrent un prix aux Data scientist obtenant les meilleures performances.', 114: '', 115: 'Conclusion', 116: \"Dans ce chapitre, nous avons pu expliquer le contexte du projet en partant de l'étude de l'existant jusqu'à la solution proposée. Ensuite, nous avons présenté notre méthodologie et les outils utilisés. Dans le prochain chapitre, nous ferons la démonstration de l'implémentation de mes données.\", 117: '', 118: '', 119: '', 120: '', 121: '', 122: '', 123: '', 124: '', 125: '', 126: '', 127: '', 128: '', 129: '', 130: '', 131: 'II. objectifs\\xa0:', 132: 'Au cours de ce chapitre, nous allons préciser les trois objectifs sur lesquels nous avons travaillé, démontrer le processus de travail et clôturer par une conclusion.', 133: '1.Text Mining:', 134: 'Pour notre premier objectif, nous avons choisi de nous concentrer sur l’objectif suivant\\xa0:  ', 135: \"Analyser les Tweets dans un laps de temps pour déterminer si les utilisateurs s'intéressent à d’autre crypto-monnaies si la valeur de Bitcoin baisse.\", 136: 'Pour cette phase, on a choisi ces trois dates :', 137: '10 Decembre 2020', 138: '', 139: '', 140: '', 141: '', 142: '', 143: '', 144: '', 145: '', 146: '', 147: '', 148: '', 149: '', 150: '', 151: '20 Avril 2021', 152: '', 153: '', 154: '', 155: '20 Juillet 2021', 156: '', 157: '', 158: '', 159: '', 160: '', 161: '', 162: \"Afin d'obtenir les datas, on a scrappé les données en utilisant API tweepy. En premier lieu, on a spécifié les champs a affiché comme le username, dscription, location, Hashtags Used.\", 163: '', 164: '', 165: '', 166: '', 167: '', 168: '', 169: '', 170: '', 171: '', 172: '', 173: '', 174: '', 175: '', 176: '', 177: '', 178: '', 179: '', 180: '', 181: '', 182: 'Deuxièment, on a effectué un traitement pour extraire les données.', 183: '', 184: '', 185: '', 186: '', 187: '', 188: '', 189: '', 190: '', 191: '', 192: '', 193: '', 194: '', 195: '', 196: '', 197: '', 198: '', 199: '', 200: '', 201: '', 202: '', 203: '', 204: '', 205: '', 206: 'Finalement, on a spécifié le hashtags qui sera le sujet de scraping ainsi que la date, et le nombre de tweets.', 207: '', 208: '', 209: 'Les données sont enregistrées sous forme d’un fichier Excel.', 210: 'Après la mise au rebut des données, nous sommes passés à la deuxième phase, à savoir le nettoyage des données.', 211: 'On a commencé par importer les champs qui nous intéressent qui sont « Hashtags\\xa0» et «\\xa0Contenu du Tweet\\xa0».', 212: '', 213: 'Ensuite, on a effectué le nettoyage des données à savoir l’enlèvement des majuscules, la ponctuation et tous les caractères spéciaux, les retours à la ligne et les mots vides. ', 214: '', 215: '', 216: '', 217: 'Ensuite, on a effectué le Stemming et la Lemmatization. ', 218: '', 219: '', 220: 'Par la suite, on a créé un WordCloud à partir des résultats.', 221: '', 222: 'Enfin, on à crée un histogramme qui nous montre les top 9 mots les plus fréquents.', 223: ' ', 224: '', 225: '', 226: '', 227: '', 228: '', 229: '', 230: '', 231: '', 232: '', 233: '', 234: '', 235: '2.Web Mining :', 236: 'Présentation de l’Objectif :  ', 237: '', 238: \"La valeur et la popularité des crypto-monnaies ont augmenté de façon exceptionnelle ces dernières années. En effet, Bitcoin a récemment gagné en popularité sur les marchés financiers. Cette  monnaie virtuelle autonome et décentralisée a fait beaucoup parler d’elle. Notre objectif initial est d'analyser les profils des utilisateurs, publiant des tweets à propos du Bitcoin, nous permettons la création des clusters afin de déterminer les différentes catégories de personnes intéressées par Bitcoin à partir du réseau social Twitter.\", 239: '', 240: 'Problème de l’objectif :', 241: '', 242: 'Pour effectuer notre objectif, nous avons décidé de travailler sur le réseau Twitter. Nous savons que ce dernier contient un nombre important de personnes les plus intéressées par le bitcoin. Ce réseau a pu accélérer son expansion avec les Super Follows, les abonnements ou encore le système de pourboire grâce à la crypto-monnaie, qui devient une des trois principales tendances clé de ce réseau social. ', 243: 'Par ailleurs le problème de twitter, les informations de l’utilisateur sont limitées, nous pouvons apercevoir uniquement la date de création et l’emplacement même si ce dernier n’existe pas forcément dans tous les profils.', 244: '', 245: 'Pour cela nous avons été amené à chercher les meilleurs sites contenant une grande communauté de la monnaie Bitcoin, nous avons trouvé que le top de la liste est reddit (crypto-currency subreddit ) avec 29 postes par jour, et nous avons pu voir aussi d’autres forums avec en moyenne une vingtaine de postes par jours, comme par exemples  CryptoInTalk, Bitcoin Forum... , mais ces résultats sont assez faibles. Nous avons donc pris le choix de rester travailler sur twitter, qui publie une centaine jusqu’à des milliers de posts par jour.', 246: '', 247: 'Meilleure solution :', 248: '', 249: 'Pour bénéficier de l’avantage de notre réseau, nous avons essayé d’utiliser la quantité des données limitées disponibles sur Twitter à propos des profils de l’utilisateur, qui nous permettra par la suite de les analyser. ', 250: 'En effet, ses données contiennent la localisation géographique ainsi que les informations concernant le compte de l’utilisateur (following, followers, total tweets, retweet counts).', 251: '', 252: \"Pour avoir des résultats plus fiables, nous allons récupérer dans un premier temps l’emplacement géographique des utilisateurs les plus intéressés par le Bitcoin, dans un second temps nous allons faire, pour chacun de ces pays, l’indicateur sentimental en général, montrant la moyenne des analyses sentimentales individuelles. Et pour finir, les analyses sur les informations du compte utilisateur, nous permettent d’obtenir un score pour chaque utilisateur. Ce score va nous informer sur la situation de l’utilisateur, qui peut être soit un leader d’opinion c’est à dire un influenceur, soit un spammeur d’opinion tentant d'induire les lecteurs en erreur en donnant des opinions indignes sur un produit ou un service afin de le promouvoir à leur réputation, et soit un utilisateur actif, ces utilisateurs ont accès à ces publications, ils peuvent aimer, commenter ou même partager les postes.\", 253: '', 254: 'Implémentation de l’objectif :', 255: '', 256: 'L’implémentation de notre projet se déroule en plusieurs étapes, tout d’abord nous allons commencer par la data preparation, puis par l’analyse des sentiments, ensuite l’analyse de localisation, l’analyse des sentiments par rapport à la localisation et enfin l’analyse d’opinion.', 257: '', 258: 'Data preparation :', 259: '', 260: 'Tout d’abord nous allons parler de la data preparation. Nous avons repris les étapes de préparation des données vues précédemment tout en faisant une jointure entre ces fichiers. Pour ce faire, nous avons commencé par l’importation du package pandas puis nous avons utilisé la fonction concat afin de concaténer les éléments. Nous avons choisi l’option “map” telle une boucle for ainsi que l’option “ignore_index” de manière à réinitialiser l’index existant dans le résultat en question. ', 261: '', 262: '', 263: '', 264: 'Analyse des sentiments :', 265: '', 266: 'Ensuite, nous avons travaillé sur les analyses de sentiments sur les données après le traitement afin de les intégrer avec les analyses des emplacements. Nous avons alors affecté des données traitées et nettoyées à une nouvelle colonne de notre Dataframe. Puis nous avons rajouté une autre colonne pour les sentiments. ', 267: '', 268: '', 269: '', 270: 'Nous récupérons ainsi seulement les valeurs des sentiments. ', 271: '', 272: '', 273: '', 274: '', 275: '', 276: '', 277: '', 278: '', 279: 'Analyse de localisation :', 280: '', 281: 'Dans cette partie, nous établissons une analyse de localisation. Celle-ci consiste à définir la fonction de récupération de la localisation grâce à l’utilisation de l’APIGeocode. Pour ce faire, nous prenons en compte les valeurs nulles et aberrantes. ', 282: '', 283: '', 284: '', 285: 'Par la suite, nous appliquons la fonction qui récupère la localisation avec la création d’une nouvelle colonne country. Ainsi, dans notre partie affichage, nous allons obtenir un pourcentage qui mettra en évidence l’état de notre traitement et nous obtiendrons également une nouvelle colonne des pays.', 286: ' ', 287: '', 288: '', 289: 'De plus, nous pouvons avoir notre première interprétation qui nous affiche un tableau de localisation avec le nombre d’individus. Nous pouvons remarquer que la  majorité des localisations des utilisateurs twitter sont invisibles. Nous constatons également que les États-Unis possèdent un résultat assez élevé de 112. Nous retrouvons aussi un chiffre aux alentours de 35 pour l’Inde et le Royaume-Uni ainsi qu’un chiffre de 20 pour l’Allemagne.', 290: '', 291: '', 292: '', 293: 'Ainsi, nous avons effectué un boxplot qui nous révèle le top 5 des pays intéressés par le bitcoin. Ce dernier nous confirme les résultats obtenus précédemment. Effectivement, nous retrouvons en première position les américains avec un nombre supérieur à 100, puis les indiens et les anglais aux alentours de 20 et 40. Enfin, les allemands et les indonésiens se retrouvent dans les derniers avec un résultat situé entre 15 et 20.', 294: '', 295: '', 296: 'Analyse des sentiments par rapport à la localisation :', 297: '', 298: 'Nous allons ensuite passer à l’analyse des sentiments par rapport à la localisation. Ce code crée une liste des 5 premiers pays pour laquelle nous calculons la moyenne des indicateurs sentimentaux de chaque pays. ', 299: '', 300: '', 301: '', 302: 'Les résultats nous confirment que de nos jours les pays asiatiques sont les plus intéressés par le bitcoin ce qui confirme nos recherches. Aussi, l’Inde constitue le pays où l’intérêt est le plus élevé. L’Asie est suivie par un pays européen qui est l’Allemagne qui se fait rattraper ensuite par les États Unis.', 303: '', 304: \"Nous constatons également que l’Inde, qui est le pays dont l'intérêt pour le bitcoin est le plus élevé, possède la valeur représentative du sentiment de l’utilisateur la plus élevée qui est de 0.186004. \", 305: 'Nous retrouvons ensuite l’Indonésie avec une valeur de 0.158661 et l’Allemagne avec un chiffre de 0.114735.', 306: ' ', 307: 'Par conséquent, nous pouvons mettre en évidence la relation entre le classement des pays et la valeur du sentiment de l’utilisateur : cette valeur positive mais qui est presque nulle évolue de la même manière que le classement des pays.', 308: ' ', 309: '', 310: '', 311: '', 312: '', 313: '', 314: '', 315: 'Analyse d’opinion : ', 316: '', 317: 'Nous allons finalement clôturer notre analyse avec l’analyse d’opinion. En effet, ce code permet d’obtenir la catégorie des utilisateurs en fonction des informations contenues dans les comptes de ces derniers. ', 318: 'En fonction des informations de followers, following, tweets et retweets, nous calculons deux scores : le scoreLeader ainsi que le scoreSpammer. Le scoreLeader représente le type de personne dominante sur Twitter, c’est-à-dire le type de personnes suivies par beaucoup de personnes et très actives, tandis que le scoreSpammer représente le type de personnes plutôt suiveuses. ', 319: '', 320: '', 321: '', 322: '', 323: 'Ensuite, avec ces deux résultats nous définissons la catégorie grâce à la fonction categoryPredict qui retourne cette dernière. Dans le cas où la personne n’est ni Leader, ni Spammer, on lui attribue la catégorie de Casual qui représente une catégorie de personnes normales, observatrices.', 324: '', 325: '', 326: '', 327: 'Ici, nous faisons l’appel de la fonction de prédiction en passant par les arguments nécessaires. Dans ce cas, à partir des 1000 individus nous obtenons 92% de personnes dites Casual, 42% de personnes Leader ainsi que 38% de Spammer. ', 328: '', 329: '', 330: '', 331: 'Finalement, nous obtenons un tableau qui nous permettra de distinguer les utilisateurs leader d’opinion. En effet, nous observons les noms d’utilisateurs des leaders, ainsi que leur nombre de followers, followings, de tweets et de retweets. ', 332: '', 333: '', 334: '', 335: 'Pour conclure, nous avons développé un code qui nous a permis de définir les pays les plus intéressés par la crypto-monnaie qu’est le Bitcoin grâce à l’utilisation des posts des utilisateurs du réseau social Twitter. Ainsi, par l’analyse des informations des internautes intéressés par le Bitcoin, nous avons pu établir une liste des pays les plus concernés par ce type de monnaie ainsi que leur chiffre d’intérêt de manière à répondre à nos objectifs principaux. ', 336: '', 337: '', 338: '', 339: '', 340: '', 341: '', 342: '', 343: '', 344: '3.Analyse des sentiments :', 345: '', 346: '', 347: '', 348: '', 349: 'Dans cette partie, on a réalisé une analyse sur les commentaires contenant', 350: 'le hashtag #bitcoin.', 351: 'dans cette partie on utilisé plusieurs librairie python comme:', 352: \"-NLTK Pour l'analyse de texte \", 353: '-Textblob une librairie Python destinée à effectuer des tâches usuelles de TAL (traitement automatisé du langage) en toute simplicité.', 354: 'Elle permet notamment d’effectuer de l’étiquetage morpho-syntaxique,de l’extraction de groupes nominaux, de l’analyse de sentiments,de la classification et de la traduction. Elle permet également d’effectuer des opérations de plus bas niveaux,', 355: ' telles que la tokenisation ou encore la lemmatisation.', 356: 'Aprés avoir utilisé nos fonction de nettoyage de données , on a calculer ', 357: 'a polarité et la subjectivité. ', 358: '', 359: '', 360: '', 361: 'Ces analyses on était fait dans le but de déduire la confiance que les gens ont par rapport au bitcoin , et on remarque que les commentaires \"positifs\" ont une valeur ', 362: 'trés importante par rapport au valeurs \"négatif\"', 363: '', 364: '', 365: '', 366: '', 367: '', 368: '', 369: '', 370: '', 371: '', 372: '', 373: '', 374: '', 375: '', 376: '', 377: '', 378: '', 379: '', 380: '', 381: '', 382: '', 383: '', 384: '', 385: '', 386: '', 387: 'III.Conclusion:', 388: '', 389: \"Étant donné que les données augmentent massivement au fil des ans, l'apprentissage des outils adéquats, l'application d'une bonne analyse et la visualisation des données de la manière la plus lisible possible constituent un raccourci vers une planification entrepreneuriale réussie.\", 390: \"Au cours de ce projet, nous avons couvert l'extraction de données, l'extraction de texte et l'analyse sentimentale. Python est un outil puissant qui a permis de créer des résultats optimaux et la meilleure prise de décision qui sera affectée par le bitcoin.\", 391: 'En utilisant ce processus, les parties intéressées par le bitcoin peuvent avoir une meilleure idée de son image publique, noter ses faiblesses et appliquer les changements nécessaires afin de rester le premier choix pour de nombreuses personnes dans le monde. ', 392: '', 393: '', 394: '', 395: '', 396: '', 397: '', 398: '', 399: '', 400: '', 401: ''})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_content.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '',\n",
       " 2: '5 ERP-BI 4',\n",
       " 3: '',\n",
       " 4: '',\n",
       " 5: '                  ',\n",
       " 6: \"Ecole Supérieure Privée d'Ingénierie\",\n",
       " 7: 'Et de Technologies',\n",
       " 8: '',\n",
       " 9: 'Rapport Advanced Learning For Text & Graph Data',\n",
       " 10: '',\n",
       " 11: '',\n",
       " 12: '',\n",
       " 13: 'Ce travail est réalisé par :',\n",
       " 14: 'CHIKHAOUI Nouha',\n",
       " 15: 'HADDAD Mohamed Aziz',\n",
       " 16: 'HATIRA Asma',\n",
       " 17: 'JBERI Samar',\n",
       " 18: 'KOOLI Youssef',\n",
       " 19: 'ZALILA Mahdi',\n",
       " 20: '',\n",
       " 21: '                                    Année académique : 2021/2022',\n",
       " 22: '',\n",
       " 23: 'Sommaire',\n",
       " 24: 'Introduction……………………………………………………………………………………………………………………………………….3',\n",
       " 25: 'I.Contexte du sujet……………..………………………………………………………………………………………………………………4',\n",
       " 26: \"1. Étude de l'existant ...................................... ........................................................................ ........... ...4\",\n",
       " 27: '2. Problématique ..................................................... .............................. ............................ ...................4 ',\n",
       " 28: '3. Solutions proposées..................................... .............................. . .............................. ............... .......4 ',\n",
       " 29: '4.Méthodologie adoptée - CRISP-DM ………....................................... ...................... ..............................5 ',\n",
       " 30: '5.Outils utilisés .............................................. .............................. .............................. ...........................6',\n",
       " 31: '5.1. Python………………………………………………………………………………………………………………………………………….6',\n",
       " 32: '5.2. Kaggle………………………………………………………………………………………………………………………………………….6',\n",
       " 33: 'II.Objectives……………………………………………..…………………………………………………………………………………………7',\n",
       " 34: '1.Text mining………………………………………………………………………………………………………………………………………7',\n",
       " 35: '1.Objective1: Text Mining…………………………………………………………………………………………………………………..7',\n",
       " 36: '2.Objective2: Web Mining………………………………………………………………………………………………………………..18',\n",
       " 37: '3.Objective3: Analyse des sentiments ………………………………………………………………………………………………28',\n",
       " 38: 'III.Conclusion…………………………………………………………………………………………………………………………………….31',\n",
       " 39: '',\n",
       " 40: '',\n",
       " 41: '',\n",
       " 42: '',\n",
       " 43: '',\n",
       " 44: '',\n",
       " 45: '',\n",
       " 46: '',\n",
       " 47: '',\n",
       " 48: '',\n",
       " 49: '',\n",
       " 50: '',\n",
       " 51: '',\n",
       " 52: '',\n",
       " 53: '',\n",
       " 54: 'Introduction',\n",
       " 55: \"Afin d'appliquer les différentes compétences que nous avons acquis au cours des derniers mois, nous sommes comme étudiants en troisième année d'ingénierie censés de réaliser un projet sur l'extraction de texte en commençant par définir trois objectifs et en travaillant jusqu'au bout.\",\n",
       " 56: \"Au cours de ce projet, nous avons eu la chance de découvrir de nouvelles technologies et d'acquérir de nouvelles compétences. Nous avons plongé dans les données et en avons tiré un livrable soigné.\",\n",
       " 57: 'Dans ce rapport, nous allons énumérer les différentes tâches effectuées ainsi que les étapes que nous avons suivies pour développer notre projet.',\n",
       " 58: \"Dans le premier chapitre, nous commencerons par une présentation du sujet, l'étude de l'existant, et les solutions proposées.\",\n",
       " 59: 'Le deuxième chapitre traitera des méthodes utilisées, tandis que le troisième chapitre traitera des objectifs et de leur démonstration. ',\n",
       " 60: 'Enfin, une conclusion résumera le travail effectué et les résultats obtenus.',\n",
       " 61: '',\n",
       " 62: '',\n",
       " 63: '',\n",
       " 64: '',\n",
       " 65: '',\n",
       " 66: '',\n",
       " 67: '',\n",
       " 68: '',\n",
       " 69: '',\n",
       " 70: '',\n",
       " 71: '',\n",
       " 72: '',\n",
       " 73: '',\n",
       " 74: '',\n",
       " 75: '',\n",
       " 76: '',\n",
       " 77: '',\n",
       " 78: 'I.Contexte du sujet',\n",
       " 79: \"Dans ce chapitre, nous commencerons par présenter l'organisme d'accueil, suivi d'une analyse approfondie du contexte du projet comprenant une étude de l'existant, de la problématique à traiter et de la solution proposée.\",\n",
       " 80: '',\n",
       " 81: '1. Étude de l’existant :',\n",
       " 82: \"Les crypto-monnaies existent depuis plus de 10 ans. Tant que beaucoup de gens n'en ont pas une connaissance approfondie, d'autres, quels que soient commerçant ou entrepreneurs dans les secteurs liés à la blockchain, y voient une opportunité.  Facebook, Twitter et Reddit sont devenus des vecteurs très populaires de connaissances sur les crypto-monnaies.\",\n",
       " 83: \"Certains des meilleurs contenus proviennent de grands acteurs, comme la société de blockchain Ripple, qui s'est attachée à partager des contenus éducatifs et des webinaires sur les crypto-monnaies avec ses près d'un million de followers sur Twitter.\",\n",
       " 84: '',\n",
       " 85: '2.  Problématique :',\n",
       " 86: \"Avec une telle quantité de suiveurs et le manque de connaissances des gens sur la culture des crypto-monnaies, il est facile de créer l’espace pour une mauvaise influence puisque les gens ont tendance à former leurs convictions à propos un certain sujet en se basant sur les opinions des gens sur les réseaux sociaux. Il est donc important de suivre l'opinion publique, surtout si la crypto-monnaie est en baisse.\",\n",
       " 87: \"La question est la suivante : si nous recueillons ces opinions, quel type d'information pouvons-nous en tirer ?\",\n",
       " 88: '',\n",
       " 89: '3.  Solutions proposés\\xa0:',\n",
       " 90: 'Le sentiment analysis s\\'agit d\\'un outil marketing puissant qui permet aux gestionnaires de bitcoin de comprendre les émotions des clients. C\\'est un facteur important lorsqu\\'il s\\'agit de la reconnaissance des bitcoins, de la fidélité des clients, de leur satisfaction, du succès de la publicité et de la promotion, et de l\\'acceptation de produit. Comprendre la psychologie des consommateurs peut aider les gestionnaires de bitcoins à modifier leur feuille de route avec plus de précision. Le terme de marketing basé sur les émotions est une expression générale qui englobe les réponses émotionnelles des clients, telles que \"positif\", \"négatif\", \"neutre\", \"négatif\", \"dégoût\", \"frustration\", etc. Comprendre la psychologie des réponses des clients peut également augmenter la valeur des bitcoins.',\n",
       " 91: \"Afin d'aborder les questions mentionnées dans la problématique, nous avons décidé de passer par trois étapes principals:\",\n",
       " 92: 'le Text Mining : scrapé des tweets , netoyer le contenu et visualiser  les termes dominant',\n",
       " 93: 'le web Mining ',\n",
       " 94: 'prediction des sentiment',\n",
       " 95: ' 4.   Adopted methodology - CRISP-DM:',\n",
       " 96: 'Pour ce projet,on a utilisé la méthodologie CRISP-DM qui signifie \"cross-industry process for data mining\". Elle fournit une approche structurée pour la planification d\\'un projet de data mining. Elle est composée essentiellement de 6 étapes :',\n",
       " 97: '1. \\tCompréhension du domaine métier',\n",
       " 98: '2. \\tCompréhension des données',\n",
       " 99: '3. \\tPréparation des données',\n",
       " 100: '4. \\tModélisation',\n",
       " 101: '5. \\tÉvaluation',\n",
       " 102: '6.           Déploiement',\n",
       " 103: '',\n",
       " 104: '',\n",
       " 105: '',\n",
       " 106: '',\n",
       " 107: '',\n",
       " 108: '5. Outils utilisés:',\n",
       " 109: ' ',\n",
       " 110: '5.1 Python:',\n",
       " 111: \"Python est un langage de programmation interprété de haut niveau à usage général. La philosophie de conception de Python met l'accent sur la lisibilité du code grâce à l'utilisation notable d'une indentation importante. Ses constructions de langage ainsi que son approche orientée objet visent à aider les programmeurs à écrire un code clair et logique pour des projets de petite et grande envergure.\",\n",
       " 112: '5.2 Kaggle :',\n",
       " 113: 'Kaggle est une plateforme web organisant des compétitions en Data science. Sur cette plateforme, les entreprises proposent des problèmes liés au Data science et offrent un prix aux Data scientist obtenant les meilleures performances.',\n",
       " 114: '',\n",
       " 115: 'Conclusion',\n",
       " 116: \"Dans ce chapitre, nous avons pu expliquer le contexte du projet en partant de l'étude de l'existant jusqu'à la solution proposée. Ensuite, nous avons présenté notre méthodologie et les outils utilisés. Dans le prochain chapitre, nous ferons la démonstration de l'implémentation de mes données.\",\n",
       " 117: '',\n",
       " 118: '',\n",
       " 119: '',\n",
       " 120: '',\n",
       " 121: '',\n",
       " 122: '',\n",
       " 123: '',\n",
       " 124: '',\n",
       " 125: '',\n",
       " 126: '',\n",
       " 127: '',\n",
       " 128: '',\n",
       " 129: '',\n",
       " 130: '',\n",
       " 131: 'II. objectifs\\xa0:',\n",
       " 132: 'Au cours de ce chapitre, nous allons préciser les trois objectifs sur lesquels nous avons travaillé, démontrer le processus de travail et clôturer par une conclusion.',\n",
       " 133: '1.Text Mining:',\n",
       " 134: 'Pour notre premier objectif, nous avons choisi de nous concentrer sur l’objectif suivant\\xa0:  ',\n",
       " 135: \"Analyser les Tweets dans un laps de temps pour déterminer si les utilisateurs s'intéressent à d’autre crypto-monnaies si la valeur de Bitcoin baisse.\",\n",
       " 136: 'Pour cette phase, on a choisi ces trois dates :',\n",
       " 137: '10 Decembre 2020',\n",
       " 138: '',\n",
       " 139: '',\n",
       " 140: '',\n",
       " 141: '',\n",
       " 142: '',\n",
       " 143: '',\n",
       " 144: '',\n",
       " 145: '',\n",
       " 146: '',\n",
       " 147: '',\n",
       " 148: '',\n",
       " 149: '',\n",
       " 150: '',\n",
       " 151: '20 Avril 2021',\n",
       " 152: '',\n",
       " 153: '',\n",
       " 154: '',\n",
       " 155: '20 Juillet 2021',\n",
       " 156: '',\n",
       " 157: '',\n",
       " 158: '',\n",
       " 159: '',\n",
       " 160: '',\n",
       " 161: '',\n",
       " 162: \"Afin d'obtenir les datas, on a scrappé les données en utilisant API tweepy. En premier lieu, on a spécifié les champs a affiché comme le username, dscription, location, Hashtags Used.\",\n",
       " 163: '',\n",
       " 164: '',\n",
       " 165: '',\n",
       " 166: '',\n",
       " 167: '',\n",
       " 168: '',\n",
       " 169: '',\n",
       " 170: '',\n",
       " 171: '',\n",
       " 172: '',\n",
       " 173: '',\n",
       " 174: '',\n",
       " 175: '',\n",
       " 176: '',\n",
       " 177: '',\n",
       " 178: '',\n",
       " 179: '',\n",
       " 180: '',\n",
       " 181: '',\n",
       " 182: 'Deuxièment, on a effectué un traitement pour extraire les données.',\n",
       " 183: '',\n",
       " 184: '',\n",
       " 185: '',\n",
       " 186: '',\n",
       " 187: '',\n",
       " 188: '',\n",
       " 189: '',\n",
       " 190: '',\n",
       " 191: '',\n",
       " 192: '',\n",
       " 193: '',\n",
       " 194: '',\n",
       " 195: '',\n",
       " 196: '',\n",
       " 197: '',\n",
       " 198: '',\n",
       " 199: '',\n",
       " 200: '',\n",
       " 201: '',\n",
       " 202: '',\n",
       " 203: '',\n",
       " 204: '',\n",
       " 205: '',\n",
       " 206: 'Finalement, on a spécifié le hashtags qui sera le sujet de scraping ainsi que la date, et le nombre de tweets.',\n",
       " 207: '',\n",
       " 208: '',\n",
       " 209: 'Les données sont enregistrées sous forme d’un fichier Excel.',\n",
       " 210: 'Après la mise au rebut des données, nous sommes passés à la deuxième phase, à savoir le nettoyage des données.',\n",
       " 211: 'On a commencé par importer les champs qui nous intéressent qui sont « Hashtags\\xa0» et «\\xa0Contenu du Tweet\\xa0».',\n",
       " 212: '',\n",
       " 213: 'Ensuite, on a effectué le nettoyage des données à savoir l’enlèvement des majuscules, la ponctuation et tous les caractères spéciaux, les retours à la ligne et les mots vides. ',\n",
       " 214: '',\n",
       " 215: '',\n",
       " 216: '',\n",
       " 217: 'Ensuite, on a effectué le Stemming et la Lemmatization. ',\n",
       " 218: '',\n",
       " 219: '',\n",
       " 220: 'Par la suite, on a créé un WordCloud à partir des résultats.',\n",
       " 221: '',\n",
       " 222: 'Enfin, on à crée un histogramme qui nous montre les top 9 mots les plus fréquents.',\n",
       " 223: ' ',\n",
       " 224: '',\n",
       " 225: '',\n",
       " 226: '',\n",
       " 227: '',\n",
       " 228: '',\n",
       " 229: '',\n",
       " 230: '',\n",
       " 231: '',\n",
       " 232: '',\n",
       " 233: '',\n",
       " 234: '',\n",
       " 235: '2.Web Mining :',\n",
       " 236: 'Présentation de l’Objectif :  ',\n",
       " 237: '',\n",
       " 238: \"La valeur et la popularité des crypto-monnaies ont augmenté de façon exceptionnelle ces dernières années. En effet, Bitcoin a récemment gagné en popularité sur les marchés financiers. Cette  monnaie virtuelle autonome et décentralisée a fait beaucoup parler d’elle. Notre objectif initial est d'analyser les profils des utilisateurs, publiant des tweets à propos du Bitcoin, nous permettons la création des clusters afin de déterminer les différentes catégories de personnes intéressées par Bitcoin à partir du réseau social Twitter.\",\n",
       " 239: '',\n",
       " 240: 'Problème de l’objectif :',\n",
       " 241: '',\n",
       " 242: 'Pour effectuer notre objectif, nous avons décidé de travailler sur le réseau Twitter. Nous savons que ce dernier contient un nombre important de personnes les plus intéressées par le bitcoin. Ce réseau a pu accélérer son expansion avec les Super Follows, les abonnements ou encore le système de pourboire grâce à la crypto-monnaie, qui devient une des trois principales tendances clé de ce réseau social. ',\n",
       " 243: 'Par ailleurs le problème de twitter, les informations de l’utilisateur sont limitées, nous pouvons apercevoir uniquement la date de création et l’emplacement même si ce dernier n’existe pas forcément dans tous les profils.',\n",
       " 244: '',\n",
       " 245: 'Pour cela nous avons été amené à chercher les meilleurs sites contenant une grande communauté de la monnaie Bitcoin, nous avons trouvé que le top de la liste est reddit (crypto-currency subreddit ) avec 29 postes par jour, et nous avons pu voir aussi d’autres forums avec en moyenne une vingtaine de postes par jours, comme par exemples  CryptoInTalk, Bitcoin Forum... , mais ces résultats sont assez faibles. Nous avons donc pris le choix de rester travailler sur twitter, qui publie une centaine jusqu’à des milliers de posts par jour.',\n",
       " 246: '',\n",
       " 247: 'Meilleure solution :',\n",
       " 248: '',\n",
       " 249: 'Pour bénéficier de l’avantage de notre réseau, nous avons essayé d’utiliser la quantité des données limitées disponibles sur Twitter à propos des profils de l’utilisateur, qui nous permettra par la suite de les analyser. ',\n",
       " 250: 'En effet, ses données contiennent la localisation géographique ainsi que les informations concernant le compte de l’utilisateur (following, followers, total tweets, retweet counts).',\n",
       " 251: '',\n",
       " 252: \"Pour avoir des résultats plus fiables, nous allons récupérer dans un premier temps l’emplacement géographique des utilisateurs les plus intéressés par le Bitcoin, dans un second temps nous allons faire, pour chacun de ces pays, l’indicateur sentimental en général, montrant la moyenne des analyses sentimentales individuelles. Et pour finir, les analyses sur les informations du compte utilisateur, nous permettent d’obtenir un score pour chaque utilisateur. Ce score va nous informer sur la situation de l’utilisateur, qui peut être soit un leader d’opinion c’est à dire un influenceur, soit un spammeur d’opinion tentant d'induire les lecteurs en erreur en donnant des opinions indignes sur un produit ou un service afin de le promouvoir à leur réputation, et soit un utilisateur actif, ces utilisateurs ont accès à ces publications, ils peuvent aimer, commenter ou même partager les postes.\",\n",
       " 253: '',\n",
       " 254: 'Implémentation de l’objectif :',\n",
       " 255: '',\n",
       " 256: 'L’implémentation de notre projet se déroule en plusieurs étapes, tout d’abord nous allons commencer par la data preparation, puis par l’analyse des sentiments, ensuite l’analyse de localisation, l’analyse des sentiments par rapport à la localisation et enfin l’analyse d’opinion.',\n",
       " 257: '',\n",
       " 258: 'Data preparation :',\n",
       " 259: '',\n",
       " 260: 'Tout d’abord nous allons parler de la data preparation. Nous avons repris les étapes de préparation des données vues précédemment tout en faisant une jointure entre ces fichiers. Pour ce faire, nous avons commencé par l’importation du package pandas puis nous avons utilisé la fonction concat afin de concaténer les éléments. Nous avons choisi l’option “map” telle une boucle for ainsi que l’option “ignore_index” de manière à réinitialiser l’index existant dans le résultat en question. ',\n",
       " 261: '',\n",
       " 262: '',\n",
       " 263: '',\n",
       " 264: 'Analyse des sentiments :',\n",
       " 265: '',\n",
       " 266: 'Ensuite, nous avons travaillé sur les analyses de sentiments sur les données après le traitement afin de les intégrer avec les analyses des emplacements. Nous avons alors affecté des données traitées et nettoyées à une nouvelle colonne de notre Dataframe. Puis nous avons rajouté une autre colonne pour les sentiments. ',\n",
       " 267: '',\n",
       " 268: '',\n",
       " 269: '',\n",
       " 270: 'Nous récupérons ainsi seulement les valeurs des sentiments. ',\n",
       " 271: '',\n",
       " 272: '',\n",
       " 273: '',\n",
       " 274: '',\n",
       " 275: '',\n",
       " 276: '',\n",
       " 277: '',\n",
       " 278: '',\n",
       " 279: 'Analyse de localisation :',\n",
       " 280: '',\n",
       " 281: 'Dans cette partie, nous établissons une analyse de localisation. Celle-ci consiste à définir la fonction de récupération de la localisation grâce à l’utilisation de l’APIGeocode. Pour ce faire, nous prenons en compte les valeurs nulles et aberrantes. ',\n",
       " 282: '',\n",
       " 283: '',\n",
       " 284: '',\n",
       " 285: 'Par la suite, nous appliquons la fonction qui récupère la localisation avec la création d’une nouvelle colonne country. Ainsi, dans notre partie affichage, nous allons obtenir un pourcentage qui mettra en évidence l’état de notre traitement et nous obtiendrons également une nouvelle colonne des pays.',\n",
       " 286: ' ',\n",
       " 287: '',\n",
       " 288: '',\n",
       " 289: 'De plus, nous pouvons avoir notre première interprétation qui nous affiche un tableau de localisation avec le nombre d’individus. Nous pouvons remarquer que la  majorité des localisations des utilisateurs twitter sont invisibles. Nous constatons également que les États-Unis possèdent un résultat assez élevé de 112. Nous retrouvons aussi un chiffre aux alentours de 35 pour l’Inde et le Royaume-Uni ainsi qu’un chiffre de 20 pour l’Allemagne.',\n",
       " 290: '',\n",
       " 291: '',\n",
       " 292: '',\n",
       " 293: 'Ainsi, nous avons effectué un boxplot qui nous révèle le top 5 des pays intéressés par le bitcoin. Ce dernier nous confirme les résultats obtenus précédemment. Effectivement, nous retrouvons en première position les américains avec un nombre supérieur à 100, puis les indiens et les anglais aux alentours de 20 et 40. Enfin, les allemands et les indonésiens se retrouvent dans les derniers avec un résultat situé entre 15 et 20.',\n",
       " 294: '',\n",
       " 295: '',\n",
       " 296: 'Analyse des sentiments par rapport à la localisation :',\n",
       " 297: '',\n",
       " 298: 'Nous allons ensuite passer à l’analyse des sentiments par rapport à la localisation. Ce code crée une liste des 5 premiers pays pour laquelle nous calculons la moyenne des indicateurs sentimentaux de chaque pays. ',\n",
       " 299: '',\n",
       " 300: '',\n",
       " 301: '',\n",
       " 302: 'Les résultats nous confirment que de nos jours les pays asiatiques sont les plus intéressés par le bitcoin ce qui confirme nos recherches. Aussi, l’Inde constitue le pays où l’intérêt est le plus élevé. L’Asie est suivie par un pays européen qui est l’Allemagne qui se fait rattraper ensuite par les États Unis.',\n",
       " 303: '',\n",
       " 304: \"Nous constatons également que l’Inde, qui est le pays dont l'intérêt pour le bitcoin est le plus élevé, possède la valeur représentative du sentiment de l’utilisateur la plus élevée qui est de 0.186004. \",\n",
       " 305: 'Nous retrouvons ensuite l’Indonésie avec une valeur de 0.158661 et l’Allemagne avec un chiffre de 0.114735.',\n",
       " 306: ' ',\n",
       " 307: 'Par conséquent, nous pouvons mettre en évidence la relation entre le classement des pays et la valeur du sentiment de l’utilisateur : cette valeur positive mais qui est presque nulle évolue de la même manière que le classement des pays.',\n",
       " 308: ' ',\n",
       " 309: '',\n",
       " 310: '',\n",
       " 311: '',\n",
       " 312: '',\n",
       " 313: '',\n",
       " 314: '',\n",
       " 315: 'Analyse d’opinion : ',\n",
       " 316: '',\n",
       " 317: 'Nous allons finalement clôturer notre analyse avec l’analyse d’opinion. En effet, ce code permet d’obtenir la catégorie des utilisateurs en fonction des informations contenues dans les comptes de ces derniers. ',\n",
       " 318: 'En fonction des informations de followers, following, tweets et retweets, nous calculons deux scores : le scoreLeader ainsi que le scoreSpammer. Le scoreLeader représente le type de personne dominante sur Twitter, c’est-à-dire le type de personnes suivies par beaucoup de personnes et très actives, tandis que le scoreSpammer représente le type de personnes plutôt suiveuses. ',\n",
       " 319: '',\n",
       " 320: '',\n",
       " 321: '',\n",
       " 322: '',\n",
       " 323: 'Ensuite, avec ces deux résultats nous définissons la catégorie grâce à la fonction categoryPredict qui retourne cette dernière. Dans le cas où la personne n’est ni Leader, ni Spammer, on lui attribue la catégorie de Casual qui représente une catégorie de personnes normales, observatrices.',\n",
       " 324: '',\n",
       " 325: '',\n",
       " 326: '',\n",
       " 327: 'Ici, nous faisons l’appel de la fonction de prédiction en passant par les arguments nécessaires. Dans ce cas, à partir des 1000 individus nous obtenons 92% de personnes dites Casual, 42% de personnes Leader ainsi que 38% de Spammer. ',\n",
       " 328: '',\n",
       " 329: '',\n",
       " 330: '',\n",
       " 331: 'Finalement, nous obtenons un tableau qui nous permettra de distinguer les utilisateurs leader d’opinion. En effet, nous observons les noms d’utilisateurs des leaders, ainsi que leur nombre de followers, followings, de tweets et de retweets. ',\n",
       " 332: '',\n",
       " 333: '',\n",
       " 334: '',\n",
       " 335: 'Pour conclure, nous avons développé un code qui nous a permis de définir les pays les plus intéressés par la crypto-monnaie qu’est le Bitcoin grâce à l’utilisation des posts des utilisateurs du réseau social Twitter. Ainsi, par l’analyse des informations des internautes intéressés par le Bitcoin, nous avons pu établir une liste des pays les plus concernés par ce type de monnaie ainsi que leur chiffre d’intérêt de manière à répondre à nos objectifs principaux. ',\n",
       " 336: '',\n",
       " 337: '',\n",
       " 338: '',\n",
       " 339: '',\n",
       " 340: '',\n",
       " 341: '',\n",
       " 342: '',\n",
       " 343: '',\n",
       " 344: '3.Analyse des sentiments :',\n",
       " 345: '',\n",
       " 346: '',\n",
       " 347: '',\n",
       " 348: '',\n",
       " 349: 'Dans cette partie, on a réalisé une analyse sur les commentaires contenant',\n",
       " 350: 'le hashtag #bitcoin.',\n",
       " 351: 'dans cette partie on utilisé plusieurs librairie python comme:',\n",
       " 352: \"-NLTK Pour l'analyse de texte \",\n",
       " 353: '-Textblob une librairie Python destinée à effectuer des tâches usuelles de TAL (traitement automatisé du langage) en toute simplicité.',\n",
       " 354: 'Elle permet notamment d’effectuer de l’étiquetage morpho-syntaxique,de l’extraction de groupes nominaux, de l’analyse de sentiments,de la classification et de la traduction. Elle permet également d’effectuer des opérations de plus bas niveaux,',\n",
       " 355: ' telles que la tokenisation ou encore la lemmatisation.',\n",
       " 356: 'Aprés avoir utilisé nos fonction de nettoyage de données , on a calculer ',\n",
       " 357: 'a polarité et la subjectivité. ',\n",
       " 358: '',\n",
       " 359: '',\n",
       " 360: '',\n",
       " 361: 'Ces analyses on était fait dans le but de déduire la confiance que les gens ont par rapport au bitcoin , et on remarque que les commentaires \"positifs\" ont une valeur ',\n",
       " 362: 'trés importante par rapport au valeurs \"négatif\"',\n",
       " 363: '',\n",
       " 364: '',\n",
       " 365: '',\n",
       " 366: '',\n",
       " 367: '',\n",
       " 368: '',\n",
       " 369: '',\n",
       " 370: '',\n",
       " 371: '',\n",
       " 372: '',\n",
       " 373: '',\n",
       " 374: '',\n",
       " 375: '',\n",
       " 376: '',\n",
       " 377: '',\n",
       " 378: '',\n",
       " 379: '',\n",
       " 380: '',\n",
       " 381: '',\n",
       " 382: '',\n",
       " 383: '',\n",
       " 384: '',\n",
       " 385: '',\n",
       " 386: '',\n",
       " 387: 'III.Conclusion:',\n",
       " 388: '',\n",
       " 389: \"Étant donné que les données augmentent massivement au fil des ans, l'apprentissage des outils adéquats, l'application d'une bonne analyse et la visualisation des données de la manière la plus lisible possible constituent un raccourci vers une planification entrepreneuriale réussie.\",\n",
       " 390: \"Au cours de ce projet, nous avons couvert l'extraction de données, l'extraction de texte et l'analyse sentimentale. Python est un outil puissant qui a permis de créer des résultats optimaux et la meilleure prise de décision qui sera affectée par le bitcoin.\",\n",
       " 391: 'En utilisant ce processus, les parties intéressées par le bitcoin peuvent avoir une meilleure idée de son image publique, noter ses faiblesses et appliquer les changements nécessaires afin de rester le premier choix pour de nombreuses personnes dans le monde. ',\n",
       " 392: '',\n",
       " 393: '',\n",
       " 394: '',\n",
       " 395: '',\n",
       " 396: '',\n",
       " 397: '',\n",
       " 398: '',\n",
       " 399: '',\n",
       " 400: '',\n",
       " 401: ''}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_content['C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\\\Rapport_text_mining.docx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Extract text and do the search\n",
    "for i in range(0, NumPages):\n",
    "    PageObj = object.getPage(i)\n",
    "    Text = PageObj.extractText()\n",
    "    if re.search(String,Text):\n",
    "         print(\"Pattern Found on Page: \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2d499775a28b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter_the_text_to_Search_here\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Found the paragraph!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             )\n\u001b[1;32m--> 860\u001b[1;33m         return self._input_request(str(prompt),\n\u001b[0m\u001b[0;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(input(\"Enter_the_text_to_Search_here\"))\n",
    "for p in document.paragraphs:\n",
    "    if pattern.search(p.text):\n",
    "        print(\"Found the paragraph!\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Did not find the paragraph :(\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter_the_text_to_Search_heredifferent tasks performed as well as the steps I followed to develop my project.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-736afc974441>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# checking if it is a file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "import os\n",
    "# assign directory\n",
    "directory = 'C:/Users/Samar/Desktop/5BI4/Stage PFE/essay'\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "pattern = re.compile(input(\"Enter_the_text_to_Search_here\"))\n",
    "for filename in os.listdir(directory):\n",
    "    \n",
    "    import aspose.words as aw\n",
    "\n",
    "# load the PDF file\n",
    "\n",
    "\n",
    "# convert PDF to Word DOCX format\n",
    "\n",
    "        \n",
    "    if filename.endswith(\".docx\") and not filename.startswith('~$'):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            doc = aw.Document(filename)\n",
    "        doc.save(\"f.docx\")\n",
    "        f = os.path.join(directory, filename)\n",
    "       \n",
    "    \n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        document = Document(f)\n",
    "        \n",
    "        import re\n",
    "        \n",
    "        \n",
    "        for para in document.paragraphs: \n",
    "            for run in paragraph.runs:\n",
    "        \n",
    "                if pattern.search(para.text):\n",
    "                \n",
    "                        run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "                        print(\"Found the paragraph in this file\" ,filename)\n",
    "                        print(para.text)\n",
    "            break\n",
    "       \n",
    "        else:\n",
    "                 print(\"Did not find the paragraph :(\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-57912953efa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# For each file we find, we need to ensure it is a .docx file before adding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#  it to our list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".docx\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".doc\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'~$'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mdocument_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from docx import Document\n",
    "\n",
    "# First, we'll create an empty list to hold the path to all of your docx files\n",
    "document_list = []       \n",
    "\n",
    "# Now, we loop through every file in the folder \"G:\\GIS\\DESIGN\\ROW\\ROW_Files\\Docx\" \n",
    "# (and all it's subfolders) using os.walk().  You could alternatively use os.listdir()\n",
    "# to get a list of files.  It would be recommended, and simpler, if all files are\n",
    "# in the same folder.  Consider that change a small challenge for developing your skills!\n",
    "for path, subdirs, files in os.walk(r\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\"): \n",
    "    for name in files:\n",
    "        # For each file we find, we need to ensure it is a .docx file before adding\n",
    "        #  it to our list\n",
    "        if filename.endswith(\".docx\") or filename.endswith(\".doc\") and not filename.startswith('~$'):\n",
    "            document_list.append(os.path.join(path, name))\n",
    "            print(document_list)\n",
    "pattern = re.compile(input(\"Enter_the_text_to_Search_here\"))\n",
    "# Now create a loop that goes over each file path in document_list, replacing your \n",
    "# hard-coded path with the variable.\n",
    "for document_path in document_list:\n",
    "    document = Document(document_path)\n",
    "    for p in document.paragraphs:\n",
    "        if pattern.search(p.text):\n",
    "            print(\"Found the paragraph!\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Did not find the paragraph :(\")\n",
    "    # Change the document being loaded each loop\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heading True for the paragraph:  Engineer Summer internship\n",
      "Heading True for the paragraph:  Summary\n",
      "Heading True for the paragraph:  Table of figures\n",
      "Heading True for the paragraph:  General Introduction\n",
      "Heading True for the paragraph:  I. General project context:\n",
      "Heading True for the paragraph:  Presentation of the host organization:\n",
      "Heading True for the paragraph:  2 Project context:\n",
      "Heading True for the paragraph:  1 .Study of the existing:\n",
      "Heading True for the paragraph:  2.\t2.Problematic:\n",
      "Heading True for the paragraph:  I don’t see any problem here: you should formulate a question that your project answers\n",
      "Heading True for the paragraph:  2.3. Proposed solutions:\n",
      "Heading True for the paragraph:  2. 4.Adopted methodology - CRISP-DM \n",
      "Heading True for the paragraph:   Conclusion\n",
      "Heading True for the paragraph:  Implemantation:\n",
      "Heading True for the paragraph:  3.1.\tUsed tools \n",
      "Heading True for the paragraph:   3.1.1.Microsoft Power BI\n",
      "Heading True for the paragraph:  3.1.2.Python \n",
      "Heading True for the paragraph:  4.Data Warehouse \n",
      "Heading True for the paragraph:  4.1.Data Warehouse modeling approach \n",
      "Heading True for the paragraph:   Bottom-Up approach by Ralph Kimball: \n",
      "Heading True for the paragraph:  II.\tData Scrapping:\n",
      "Heading True for the paragraph:  1.Data integration using power BI\n",
      "Heading True for the paragraph:   \n",
      "Heading True for the paragraph:  2. Data warehouse model:\n",
      "Heading True for the paragraph:  III.Text Mining\n",
      "Heading True for the paragraph:  IV.Data Visualization with Power BI\n",
      "Heading True for the paragraph:  V.conclusion :\n"
     ]
    }
   ],
   "source": [
    "#Iterate over paragraphs\n",
    "for paragraph in document.paragraphs:\n",
    "    \n",
    "    #Perform the below logic only for paragraph content which does not have it's native style as \"Heading\"\n",
    "    if \"Heading\" not in paragraph.style.name:\n",
    "\n",
    "        #Start of by initializing an empty string to store bold words inside a run\n",
    "        runboldtext = ''\n",
    "\n",
    "        # Iterate over all runs of the current paragraph and collect all the words which are bold into the varible \"runboldtext\"\n",
    "        for run in paragraph.runs:                        \n",
    "            if run.bold:\n",
    "                runboldtext = runboldtext + run.text\n",
    "\n",
    "        # Now check if the value of \"runboldtext\" matches the entire paragraph text. If it matches, it means all the words in the current paragraph are bold and can be considered as a heading\n",
    "        if runboldtext == str(paragraph.text) and runboldtext != '':\n",
    "            print(\"Heading True for the paragraph: \",runboldtext)\n",
    "            style_of_current_paragraph = 'Heading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-197ee34aee41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\Report_Samar_Jberi.docx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Gather a list of the paragraphs using the respective heading styles.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mparasStyleHeading1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparagraphs_by_style_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Heading 1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'aw' is not defined"
     ]
    }
   ],
   "source": [
    "# Load document\n",
    "doc = aw.Document(\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\Report_Samar_Jberi.docx\")\n",
    "\n",
    "# Gather a list of the paragraphs using the respective heading styles.\n",
    "parasStyleHeading1 = paragraphs_by_style_name(doc, \"Heading 1\")\n",
    "parasStyleHeading3 = paragraphs_by_style_name(doc, \"Heading 3\")\n",
    "\n",
    "# Use the first instance of the paragraphs with those styles.\n",
    "startPara1 = parasStyleHeading1[0]\n",
    "endPara1 = parasStyleHeading3[0]\n",
    "\n",
    "# Extract the content between these nodes in the document. Don't include these markers in the extraction.\n",
    "extractedNodes = extract_content(startPara1, endPara1, False)\n",
    "\n",
    "# Generate document containing extracted content.\n",
    "dstDoc = generate_document(doc, extractedNodes)\n",
    "\n",
    "# Save document.\n",
    "dstDoc.save(\"extract_content_between_paragraphs_based_on-Styles.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aspose-words\n",
      "  Downloading aspose_words-22.3.0-py3-none-win_amd64.whl (50.8 MB)\n",
      "     ---------------------------------------- 50.8/50.8 MB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: aspose-words\n",
      "Successfully installed aspose-words-22.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install aspose-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter_the_text_to_Search_hereThen proceeded to present my methodology\n",
      "Found the paragraph!\n",
      "['', '', '', '', '', \"                      Ecole Supérieure Privée d'Ingénierie\", 'Et de Technologies', '', 'Engineer Summer internship', '', '', '', '', '', '', '', '', 'Developed by Samar Jberi', 'Supervised by Mohammed Tlili', 'August 6th to September 30th, 2021', '', 'Academic Year: 2021/2022', '', '', '', '', '', 'Summary', 'General Introduction……………………………………………………………………………………………………………………………….5', 'I           General project context………………………………………..………………………………………………………………………6', '1.\\tPresentation of the host organization……………………………..………………………………………………………….6', '2.\\t Project context……………………………………………………………………………..……………………………………..…….6', '2.1.\\t Study of the existing…………………..…………………………………………………….……………….………….…….....…6 ', '2.2.\\tProblematic………………………………………………………………………………………………….……………...…………....7', '2.3.\\tProposed solutions………………...…………………………………………………….……………………………………...……7', '2.4.\\tAdopted methodology - CRISP-DM……..…………………………………………….…………………………….………….7', '3\\tImplementation……………………………………………………………………………………….……………………………...…9', '3.1.\\tUsed tools …………………………………………………………………………………………………….…………………………...9', '3.1.1.\\tMicrosoft Power BI…………………………………………………………………………………….……………………………….9', '3.1.2.\\tPython…………………………………………………………………………………………………………………………………….....9', '4.           Data Warehouse ……………………………………………………………………….……………………………………………….9', '4.1.        Data Warehouse modeling approach..............................................................................................9 ', 'II.             Data Scraping...............................................................................................................................10', '1.         Data integration using Power BI………………………………………………………………………………………………....11', '1.2.        Data warehouse model.................................................................................................................16', 'III.            Text Mining……………………………………………………………………………………………………………………………..18', 'IV.\\tData visualization …..………………………………………………………………………………………………………………..23', 'V.\\tConclusion……………….……………………………………………………………………………………………………………....26', '', '', '', '', '', 'Table of figures', '', 'Figure 1: Esprit Logo……………………………………………………………………………………….6', 'Figure 2: CRISP-DM Methodology………………………………………………………………………...8', 'Figure 3: Bottom-Up  approach by Ralph Kimball………………………………………………………..10', 'Figure 4: Connecting to the web Page…………………………………………………………………….11', 'Figure 5: Data picking and storing in csv file……………………………………………………………..11', 'Figure 6: Calendar Table  ………………………………………………………………………………....12                                                             ', 'Figure 7: Posts_Fact_table..………………………………………………………………………….……13', 'Figure 8: Revieweres LookUp…………………………………………………………………………….13', 'Figure 9: Skills LookUp_Table……………………………………………………………………………14', 'Figure 10: What They_Do LookUp table…………………………………………………………………14', 'Figure 11: Where_They_live lookUp Table……………………………………………………………....15', 'Figure 12: Where_They_Studied LookUp Table………………………………………………………….15', 'Figure 13: LinkedIn Warehouse…………………………………………………………………………..16', 'Figure 14: Facebook Warehouse ………………………………………………………………………….17                                                     ', 'Figure15: Cleaning data …………………………………………………………………………………..18 ', 'Figure 16: Study of frequency…….………………………………………………………………………19                                                ', 'Figure 17: Frequency plot of single terms…………………………………………………………..…....19', 'Figure 19: Frequency Plot of coupled term……………………………………………………………….20', 'Figure 18: Content filtering………………………………………………………………………………..20', 'Figure 20: Word Stemming…………………………………………………….……………………….....21                    ', 'Figure 21: Words_Clouds ………………………………………………………………………………...21      ', 'Figure 22: Bar_Plot………………………………………………………………………………………..22                                                          ', 'Figure 23: Data_training…………………………………………………………………………………..22', 'Figure 24: Facebook_dashboard_1………………………………………………………………………..23', 'Figure25: facebook_Dashboard_2………………………………………………………………………...23', 'Figure 26:Skills………………………………...………………………………………………………….24', 'Figure 27:Where_They_Live……………………………………………………………………………...24', 'Figure 28:Where_They_Studied…………………………………………………………………………..25', 'Figure 29:What_They_Do………………………………………………………………………………...25', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'General Introduction', 'In order to apply the different skills we have acquired during the past couple of years, students conduct a summer internship after their second year of engineering studies.', 'Supported by the Private Higher School of Engineering and Technologies Esprit, I was offered the opportunity to work on the subject entitled \"Social Network analysis\" that took place from 06/08/2020 to 30/09/2021, under the supervision and help of Mr. Mohammed Tlili.', 'During this internship I had the chance to discover new technologies and acquire new skills.', 'Dived into the data and created a neat deliverable out of it.', 'In this report, I will list the different tasks performed as well as the steps I followed to develop my project.', 'In the first chapter, I will start by a presentation of the subject, study of the existing, and proposed solutions.', 'The second chapter will deal with the text mining part of my project, while the third chapter will deal with data visualization. ', 'Finally, a conclusion will summarize the work done and the results obtained.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'General project context:', 'In this chapter I will start by presenting the host organization, followed by a deep analysis of the project context including a study of the existing, the problematic at hand and the proposed solution. Lastly, I will specify the adopted methodology.', '', 'Presentation of the host organization:', '', '', '', '', '', '', 'Founded in 2003, Esprit has managed to become an internationally recognized establishment.', 'In addition to being accredited by the Tunisian Ministry of Higher Education and Scientific Research, the quality of Esprit education has become well renown over the years.', 'Following the lead of bigger higher education facilities such as MIT, Esprit builds the pedagogy of engineering training around professional scenarios. Esprit aims towards excellence.', 'And for this exact reason Esprit cares deeply about its public image, collecting and taking into consideration all the feedback in order to continuously improve its programs and keep up with the expectations to offer the best college experience possible for its students and employees.', '  Project context:', '2.1 Study of the existing:', 'Nowadays, people tend to share so much of themselves and their feelings on internet and social networks.', 'This usually leads to communicate large amount of data.', 'these data can be very useful if it was correctly analyzed.', 'And since Esprit is one of the biggest private schools, there are lots of unexploited data that can make a huge difference in the University growth.', ' ', '', '2.2   Problematic:', 'The educational system has developed to include198 public higher education institution versus 63 private institutions according to a report conducted by the British council called \"Education in North Africa\" in 2014.', 'the numbers of students enrolled in higher education has experienced a significant boost from over 2.6% in 1974 to 35.2% by 2015.', 'With such tight competition within the private sector, each private institute aims to step up in order to maintain its success and notoriety', 'The question asked, how can these data help in the growth of Esprit against other competitors in the private sector?', '', '2.3   Proposed solutions:', 'Based on identified needs and my knowledge about the subject since I’m an Esprit student myself, I was assigned the project “Social Network analysis Esprit” in pursuance of finding solutions for the issues mentioned in the problematic.', 'Since every private facility relies heavily on its public image and, I realized that it is an essential starting point to consult the public reviews online about Esprit, study its foundation and followers and evaluate the interactions that take place on the social networks in order to identify and solve the most frequent problems.', 'To serve this purpose, I had to go through several steps:', '•\\tScrapping the most relevant information (followers, employees, comments, interactions…) about Esprit on social networks (Facebook, LinkedIn)', '•\\tShaping and cleaning the data, establishing relationships between the different data.', '•\\tText Mining to detect the nature of the content and the most frequent searched terms.', '•\\tUsing Data Visualization tools to classify posts based on different criteria to make the data more legible and have a general grasp on the image of Esprit.', '', '  2. 4.   Adopted methodology - CRISP-DM', 'on this project I used CRISP-DM methodology which stands for cross-industry process for data mining. ', 'it provides a structured approach to planning a data mining project.', \"it's composed essentially of 6 stages:\", '1. \\tBusiness understanding', '2. \\tData understanding', '3. \\tData preparation', '4. \\tModeling', '5. \\tEvaluation', '6.        Deployment', ' ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Conclusion', 'In this chapter, I got to explain the project’s context starting from the study of the existing to the proposed solution. Then proceeded to present my methodology. In the next chapter, I will be demonstrating the implementation of my data.']\n",
      "Did not find the paragraph :(\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "level_from_style_name = {f'Heading {i}': i for i in range(10)}\n",
    "\n",
    "def format_levels(cur_lev):\n",
    "    levs = [str(l) for l in cur_lev if l != 0]\n",
    "    return '.'.join(levs)  # Customize your format here\n",
    "\n",
    "d = docx.Document('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\Report_Samar_Jberi.docx')\n",
    "pattern = re.compile(input(\"Enter_the_text_to_Search_here\"))\n",
    "current_levels = [0] * 10\n",
    "full_text = []\n",
    "\n",
    "for p in d.paragraphs:\n",
    "    if p.style.name not in level_from_style_name:\n",
    "        full_text.append(p.text)\n",
    "    else:\n",
    "        level = level_from_style_name[p.style.name]\n",
    "        current_levels[level] += 1\n",
    "        for l in range(level + 1, 10):\n",
    "            current_levels[l] = 0\n",
    "        full_text.append(format_levels(current_levels) + ' ' + p.text)\n",
    "         \n",
    "    if pattern.search(p.text):\n",
    "            print(\"Found the paragraph!\")\n",
    "            print(full_text)   \n",
    "else:\n",
    "    print(\"Did not find the paragraph :(\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter_the_text_to_Search_herethe subject entitled \"Social Network analysis\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'paragraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-fcfc33661791>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Iterate over all runs of the current paragraph and collect all the words which are bold into the varible \"runboldtext\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mruns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mrunboldtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunboldtext\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'paragraph' is not defined"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "level_from_style_name = {f'Heading {i}': i for i in range(10)}\n",
    "\n",
    "def format_levels(cur_lev):\n",
    "    levs = [str(l) for l in cur_lev if l != 0]\n",
    "    return '.'.join(levs)  # Customize your format here\n",
    "\n",
    "d = docx.Document('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\Report_Samar_Jberi.docx')\n",
    "pattern = re.compile(input(\"Enter_the_text_to_Search_here\"))\n",
    "current_levels = [0] * 10\n",
    "full_text = []\n",
    "\n",
    "for p in d.paragraphs:\n",
    "    if p.style.name not in level_from_style_name:\n",
    "        full_text.append(p.text)\n",
    "    else:\n",
    "        level = level_from_style_name[p.style.name]\n",
    "        current_levels[level] += 1\n",
    "        for l in range(level + 1, 10):\n",
    "            current_levels[l] = 0\n",
    "        full_text.append(format_levels(current_levels) + ' ' + p.text)\n",
    "         \n",
    "    if pattern.search(p.text):\n",
    "            print(\"Found the paragraph!\")\n",
    "            print(full_text)   \n",
    "else:\n",
    "    print(\"Did not find the paragraph :(\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<docx.styles.styles.Styles at 0x1cc0a8ade50>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "styles = document.styles\n",
    "styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<docx.section._Header at 0x1cc0dae3550>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = Document()\n",
    "section = document.sections[0]\n",
    "header = section.header\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocxContent(docx_reader=DocxReader(), docx2python_kwargs={'docx_filename': 'C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\\\Report_Samar_Jberi.docx', 'image_folder': None, 'html': True, 'paragraph_styles': False, 'extract_image': None, 'docx_context': DocxReader()})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docx2python import docx2python\n",
    "\n",
    "# extract docx content with basic font styles converted to html\n",
    "docx2python('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\Report_Samar_Jberi.docx', html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2pythonNote: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading docx2python-2.0.4-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (from docx2python) (4.6.1)\n",
      "Installing collected packages: docx2python\n",
      "Successfully installed docx2python-2.0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install docx2python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_result = docx2python(\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay\\Test.docx\",extract_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx2python import docx2python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_get_runs',\n",
       " 'body_runs',\n",
       " 'core_properties',\n",
       " 'document',\n",
       " 'document_runs',\n",
       " 'docx2python_kwargs',\n",
       " 'docx_reader',\n",
       " 'endnotes_runs',\n",
       " 'footer_runs',\n",
       " 'footnotes_runs',\n",
       " 'header_runs',\n",
       " 'html_map',\n",
       " 'images',\n",
       " 'officeDocument_runs',\n",
       " 'properties',\n",
       " 'save_images',\n",
       " 'text']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Column Head'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph at depth 4\n",
    "doc_result.body[5][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Column Head']]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result.document[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['', ''], ['', '']]]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result.endnotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-ec1dbf158957>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfooter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "doc_result.footer[1][0:][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-e930da28fb70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheader_runs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "doc_result.header_runs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "def iter_headings(paragraphs):\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph.style.name.startswith('Heading'):\n",
    "            yield paragraph\n",
    "\n",
    "for heading in iter_headings(document.paragraphs):\n",
    "    print (heading.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Keywords:  \n",
      "\n",
      "\n",
      "1 1\n",
      "1.1 .\n",
      "\n",
      "1.1.1 .\n",
      "  ()\n",
      "1.1.1.1 .\n",
      "  ()\n",
      "1\n",
      "Tables\n",
      "Table 1\n",
      "Note:  \n",
      "\n",
      "Figure 1. \n",
      "For more information about all elements of APA formatting, please consult the APA Style Manual, 6th Edition.\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "level_from_style_name = {f'Heading {i}': i for i in range(10)}\n",
    "\n",
    "def format_levels(cur_lev):\n",
    "    levs = [str(l) for l in cur_lev if l != 0]\n",
    "    return '.'.join(levs)  # Customize your format here\n",
    "\n",
    "d = docx.Document('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test.docx')\n",
    "\n",
    "current_levels = [0] * 10\n",
    "full_text = []\n",
    "\n",
    "for p in d.paragraphs:\n",
    "    if p.style.name not in level_from_style_name:\n",
    "        full_text.append(p.text)\n",
    "    else:\n",
    "        level = level_from_style_name[p.style.name]\n",
    "        current_levels[level] += 1\n",
    "        for l in range(level + 1, 10):\n",
    "            current_levels[l] = 0\n",
    "        full_text.append(format_levels(current_levels) + ' ' + p.text)\n",
    "\n",
    "for l in full_text:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'docx' has no attribute 'ht'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-bf6015f335ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdocx\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mht\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'docx' has no attribute 'ht'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "\n",
      "Keywords:  \n",
      "\n",
      "\n",
      "2 1\n",
      "2.1 .\n",
      "\n",
      "2.1.1 .\n",
      "  ()\n",
      "2.1.1.1 .\n",
      "  ()\n",
      "1\n",
      "Tables\n",
      "Table 1\n",
      "Note:  \n",
      "\n",
      "Figure 1. \n",
      "For more information about all elements of APA formatting, please consult the APA Style Manual, 6th Edition.\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "d = docx.Document('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test.docx')\n",
    "\n",
    "level_from_style_name = {f'Heading {i}': i for i in range(10)}\n",
    "for p in d.paragraphs:\n",
    "    if p.style.name not in level_from_style_name:\n",
    "        full_text.append(p.text)\n",
    "    else:\n",
    "        level = level_from_style_name[p.style.name]\n",
    "        current_levels[level] += 1\n",
    "        for l in range(level + 1, 10):\n",
    "            current_levels[l] = 0\n",
    "        full_text.append(format_levels(current_levels) + ' ' + p.text)\n",
    "\n",
    "for l in full_text:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "document = Document('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test.docx')\n",
    "\n",
    "for paragraph in document.paragraphs:\n",
    "    if paragraph.style.name == 'Heading 5':\n",
    "        print(paragraph.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aspose-words in c:\\programdata\\anaconda3\\lib\\site-packages (22.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install aspose-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<aspose.words.saving.SaveOutputParameters object at 0x0000025E6B20F690>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import aspose.words as aw\n",
    "\n",
    "# load the PDF file\n",
    "doc = aw.Document(\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Rapport safé.pdf\")\n",
    "\n",
    "# convert PDF to Word DOCX format\n",
    "doc.save(\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/pdf-to-word.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\lib\\site-packages (4.5.5.64)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\samar\\appdata\\roaming\\python\\python38\\site-packages (from opencv-python) (1.19.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Using cached pytesseract-0.3.9-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (8.0.1)\n",
      "Collecting packaging>=21.3\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=21.3->pytesseract) (2.4.7)\n",
      "Installing collected packages: packaging, pytesseract\n",
      "Successfully installed packaging-21.3 pytesseract-0.3.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "  WARNING: The script pytesseract.exe is installed in 'C:\\Users\\Samar\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --user pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tesseract\n",
      "  Downloading tesseract-0.1.3.tar.gz (45.6 MB)\n",
      "     ---------------------------------------- 45.6/45.6 MB 2.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: tesseract\n",
      "  Building wheel for tesseract (setup.py): started\n",
      "  Building wheel for tesseract (setup.py): finished with status 'done'\n",
      "  Created wheel for tesseract: filename=tesseract-0.1.3-py3-none-any.whl size=45562576 sha256=50d06a015cabce3f5a017f70fbaca73f879a827e3ebcf4571ca7385fd9a1ee9a\n",
      "  Stored in directory: c:\\users\\samar\\appdata\\local\\pip\\cache\\wheels\\7f\\a1\\69\\fabe07004553a36d818e4657fed410daf525fe1ae161f469d3\n",
      "Successfully built tesseract\n",
      "Installing collected packages: tesseract\n",
      "Successfully installed tesseract-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text recognition\n",
    "import cv2\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image\n",
    "img = cv2.imread('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test/Images/fig.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "config = ('-l eng  --psm 8 --oem 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pytesseract.image_to_string(img, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytessercat\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Users/Samar/AppData/Local/Programs/Tesseract-OCR/tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bh', '']\n"
     ]
    }
   ],
   "source": [
    "# print text\n",
    "text = text.split('\\n')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Proxy error(UnsupportedFileFormatException): Unsupported file format: Unknown",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-ea921f0200e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# load the PDF file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/2016_movies.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# convert PDF to Word DOCX format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Proxy error(UnsupportedFileFormatException): Unsupported file format: Unknown"
     ]
    }
   ],
   "source": [
    "import aspose.words as aw\n",
    "\n",
    "# load the PDF file\n",
    "doc = aw.Document(\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/2016_movies.xlsx\")\n",
    "\n",
    "# convert PDF to Word DOCX format\n",
    "doc.save(\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/excel-to-word.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Excel file\n",
    "workbook = Workbook(\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/2016_movies.xlsx\")\n",
    "\n",
    "# Convert Excel to PDF\n",
    "workbook.save(\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/xlsx-to-pdf.pdf\", SaveFormat.PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aspose-cells in c:\\programdata\\anaconda3\\lib\\site-packages (22.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement v8.5.2 (from versions: none)\n",
      "ERROR: No matching distribution found for v8.5.2\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install aspose-cells  v8.5.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl.workbook.workbook import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PdfSaveOptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5fad44934217>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Create and set PDF options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpdfOptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPdfSaveOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mpdfOptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetCompliance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPdfCompliance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPDF_A_1_B\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PdfSaveOptions' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Excel file\n",
    "workbook = Workbook(\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Database.xlsx\")\n",
    "\n",
    "# Create and set PDF options\n",
    "pdfOptions = PdfSaveOptions()\n",
    "pdfOptions.setCompliance(PdfCompliance.PDF_A_1_B)\n",
    "  \n",
    "# Convert Excel to PDF\n",
    "workbook.save(\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Database-to-pdf.pdf\", pdfOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Aspose.Cells in c:\\programdata\\anaconda3\\lib\\site-packages (22.3.0)\n",
      "Collecting Version\n",
      "  Using cached version-0.1.1.tar.gz (2.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [8 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\Samar\\AppData\\Local\\Temp\\pip-install-npnip5gd\\version_d818f4bc60cc4499a9a4119501e7d39b\\setup.py\", line 4, in <module>\n",
      "      from version import __version__\n",
      "    File \"C:\\Users\\Samar\\AppData\\Local\\Temp\\pip-install-npnip5gd\\version_d818f4bc60cc4499a9a4119501e7d39b\\version.py\", line 2, in <module>\n",
      "      from itertools import izip_longest\n",
      "  ImportError: cannot import name 'izip_longest' from 'itertools' (unknown location)\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "WARNING: Ignoring invalid distribution -ymupdf (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ackaging (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install Aspose.Cells Version 22.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Attempt to create Java package 'com' without jvm",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4c3fd39f1f64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0masposecells\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0masposecells\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWorkbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSaveFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPdfSaveOptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPdfCompliance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\asposecells\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjpype\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maspose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStreamBuffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mJImplementationFor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"com.aspose.cells.wrapper.StreamBuffer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_StreamBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\jpype\\imports.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, name, path, target)\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_JDOMAINS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Attempt to create Java package '%s' without jvm\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;31m# Check for aliases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Attempt to create Java package 'com' without jvm"
     ]
    }
   ],
   "source": [
    "import asposecells\n",
    "from asposecells.api import Workbook, SaveFormat, PdfSaveOptions, PdfCompliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import docx NOT python-docx\n",
    "import docx\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "  \n",
    "# Create an instance of a word document\n",
    "doc = docx.Document()\n",
    "  \n",
    "# Add a Title to the document \n",
    "doc.add_heading('GeeksForGeeks', 0)\n",
    "  \n",
    "# Creating paragraph with some content\n",
    "para = doc.add_paragraph('''GeeksforGeeks is a Computer Science portal for geeks.''')\n",
    "  \n",
    "# Adding more content to paragraph and highlighting them\n",
    "para.add_run(''' It contains well written, well thought and well-explained '''\n",
    "            ).font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "  \n",
    "# Adding more content to paragraph\n",
    "para.add_run('''computer science and programming articles, quizzes etc.''')\n",
    "  \n",
    "# Now save the document to a location \n",
    "doc.save('gfg.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "\n",
    "for paragraph in document.paragraphs:\n",
    "    if 'created a neat deliverable out of it' in paragraph.text:\n",
    "        for run in paragraph.runs:\n",
    "            if 'created a neat deliverable out of it' in run.text:\n",
    "                    run.font.highlight_color = WD_COLOR_INDEX.YELLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter_the_text_to_Search_heredata and created a neat deliverable \n",
      "re.compile('data and created a neat deliverable ')\n",
      "Found the paragraph!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(input(\"Enter_the_text_to_Search_here\"))\n",
    "print(pattern)\n",
    "for p in document.paragraphs:\n",
    "    \n",
    "    if pattern.search(p.text):\n",
    "        p.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "        \n",
    "    print(\"Found the paragraph!\")\n",
    "    print(p.text)\n",
    "    document.save('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Report_Samar_Jberi.docx') \n",
    "    break\n",
    "else:\n",
    "    print(\"Did not find the paragraph :(\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "\n",
    "for paragraph in document.paragraphs:\n",
    "    if 'to work on the subject entitled' in paragraph.text:\n",
    "        for run in paragraph.runs:\n",
    "            if 'to work on the subject entitled' in run.text:\n",
    "                    run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "document.save('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Report_Samar_Jberi.docx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter_the_text_to_Search_hereopportunity to work on the subject entitled\n"
     ]
    }
   ],
   "source": [
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "pattern = re.compile(input(\"Enter_the_text_to_Search_here\"))\n",
    "for p in document.paragraphs:\n",
    "    if str(pattern.search(p.text)) in p.text:\n",
    "        for run in p.runs:\n",
    "            if p.text in run.text:\n",
    "                    run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "document.save('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Report_Samar_Jberi.docx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter_the_text_to_Search_hereopportunity to work on the subject entitled\n",
      "C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test\\Rapport_text_mining.docx\n",
      "C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test\\Report_Samar_Jberi.docx\n",
      "C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test\\Test.docx\n"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "import os\n",
    "import re\n",
    "# assign directory\n",
    "directory = 'C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test'\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "pattern = re.compile(input(\"Enter_the_text_to_Search_here\"))\n",
    "for filename in os.listdir(directory):\n",
    "    if not filename.startswith('~$'):\n",
    "        f = os.path.join(directory, filename)\n",
    "        print(f)\n",
    "    \n",
    "   \n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        document = Document(f)\n",
    "        import re\n",
    "        \n",
    "    \n",
    "    for p in document.paragraphs:\n",
    "        if str(pattern.search(p.text)) in p.text:\n",
    "            for run in p.runs:\n",
    "                if p.text in run.text:\n",
    "                    run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "                    print('got here')\n",
    "document.save('C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Report_Samar_Jberi.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter_the_text_to_Search_hereI had the chance to discover new technologies\n",
      "C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test\\Images\n",
      "C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test\\Rapport_text_mining.docx\n",
      "Did not find the paragraph :(\n",
      "C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test\\Report_Samar_Jberi.docx\n",
      "Found the paragraph in this file Report_Samar_Jberi.docx\n",
      "During this internship I had the chance to discover new technologies and acquire new skills.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'WD_COLOR_INDEX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-91207a899f70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpara\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mruns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mpara\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                         \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhighlight_color\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWD_COLOR_INDEX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYELLOW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                         \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WD_COLOR_INDEX' is not defined"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "import os\n",
    "import re\n",
    "# assign directory\n",
    "directory = 'C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test'\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "pattern = re.compile(input(\"Enter_the_text_to_Search_here\"))\n",
    "for filename in os.listdir(directory):\n",
    "    if not filename.startswith('~$'):\n",
    "        f = os.path.join(directory, filename)\n",
    "    \n",
    "    print(f)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        document = Document(f)\n",
    "        import re\n",
    "        \n",
    "        \n",
    "        for para in document.paragraphs: \n",
    "        \n",
    "        \n",
    "            if pattern.search(para.text):\n",
    "                print(\"Found the paragraph in this file\" ,filename)\n",
    "                print(para.text)\n",
    "                for run in para.runs:\n",
    "                    if para.text in run.text:\n",
    "                        run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "                        document.save(f) \n",
    "                break\n",
    "       \n",
    "        else:\n",
    "                 print(\"Did not find the paragraph :(\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "#extract text \n",
    "text = docx2txt.process(r\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test/Report_Samar_Jberi.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page segmentation mode: default , OCR engine mode : default\n",
    "myconfig =r\" --psm 3 --oem 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 7\n",
      "| Début |\n",
      "\n",
      "Ld\n",
      "7\n",
      "\\\n",
      "Sources d’ élément Eléments Eléments Destinataires des\n",
      "dentrée dentrée éléments de sortie\n",
      "i T\n",
      "|\n",
      "[PROCESSUS AMONT, | ! MATIERE, I | |PROCESSUS AVAL, |\n",
      "|par exemple au | ENERGIE, | | ENERGIE, | | par exemple au |\n",
      "Iniveau des | | INFORMATIONS, | |INFORMATIONS, | [niveau des clfents |\n",
      "|prestataires (internes| | Par exemple sous} | par exemple sous | I (Internes ou |\n",
      "lou externes), au | | forme de | forme de produit, | ‘externes), au niveau |\n",
      "|niveau des clients, au | | matériaux, | | service, décision | | d'autres parties |\n",
      "Iniveau d'autres | ressources, | | | lintéressées\n",
      "| parties intéressées | | exigences | i I jpertinentes i\n",
      "[pertinentes fo 4 po 4 po 4\n",
      "\n",
      "Maitrise possible et points\n",
      "\n",
      "de contrdle pour surveiller\n",
      "et mesurer les performances\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text= pytesseract.image_to_string(PIL.Image.open(\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test/Images/fig.png\"),config = myconfig)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">\n",
      "\n",
      "Se former autrement\n",
      "\n",
      "‘Be Country Hierarchy\n",
      "Country\n",
      "city\n",
      "\n",
      "What TheyDo\n",
      "\n",
      "5 Number\n",
      "\n",
      "Sl skilskey\n",
      "\n",
      "© aboutlt\n",
      "FA AboutEspritstuft ta\n",
      "FA skalskey\n",
      "Fl wrpkey\n",
      "Fl wrikey\n",
      "Fl wrskey\n",
      "i aiviaings\n",
      "\n",
      "7 whereTheyStudied\n",
      "I Number\n",
      " WhereTheyStudied\n",
      "\n",
      "3 whattheyDo q\n",
      "5 wrokey\n",
      "\n",
      "1 wrskey\n",
      "\n",
      "Reviewers LookUp\"\n",
      "\n",
      "Full Name\n",
      "Gender\n",
      "- =\n",
      "Nationality\n",
      "‘Comment_age Recommendation\n",
      "Date Reviewerskey\n",
      "Datekey Previous year reviewers\n",
      "Day Name Total of Facebook revie...\n",
      "Month Name\n",
      "Start of Month\n",
      "Start of Year\n",
      "Year\n",
      "\n",
      "‘Ys Month Name Hierarchy\n",
      "\n",
      "Month Name\n",
      "\n",
      "Start of Month Posts Data Fact 9°\"\n",
      "\n",
      "1 ‘Comments\n",
      "‘Commentlype\n",
      "\n",
      "Datekey\n",
      "< * Likes\n",
      "\n",
      "Postkey\n",
      "\n",
      "reacting Comments\n",
      "Reviewerskey\n",
      "AVG Interaction\n",
      "\n",
      "\n",
      "In [62]:\n",
      "\n",
      "In [63]:\n",
      "\n",
      "out [63]:\n",
      "\n",
      "In [64]:\n",
      "\n",
      "In [184]:\n",
      "\n",
      "In [185]:\n",
      "\n",
      "In [186]:\n",
      "\n",
      "In [187]:\n",
      "\n",
      "Text Mi\n",
      "\n",
      "#Loading NLTK\n",
      "import nltk\n",
      "\n",
      "nltk.download('punkt')\n",
      "\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data] C:\\Users\\Samar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Package punkt is already up-to-date!\n",
      "\n",
      "True\n",
      "\n",
      "from nltk.tokenize import sent_tokenize\n",
      "\n",
      "text = ListNames.to_string()\n",
      "\n",
      "#remove Punctuation of of the Text\n",
      "import re\n",
      "\n",
      "text = re.sub(r'[*\\w\\s]', '', text)\n",
      "tokenized_text=sent_tokenize(text)\n",
      "\n",
      "#convert text into tokenized words\n",
      "from nltk.tokenize import word_tokenize\n",
      "tokenized_word=word_tokenize(text)\n",
      "\n",
      "In [97]: from nltk.probability import FreqDist\n",
      "fdist = FregDist(tokenized_word)\n",
      "print(fdist)\n",
      "\n",
      "<FreqDist with 869 samples and 2284 outcomes>\n",
      "\n",
      "In [98]: #most_frequent_words\n",
      "fdist.most_common(1@)\n",
      "\n",
      "out[9s]: [('a’, 1¢@),\n",
      "(de, 96),\n",
      "Cet’, 94),\n",
      "(CESPRIT, 98),\n",
      "CEcole', 98),\n",
      "Sup\", 98),\n",
      "Privée’, 99),\n",
      "(dingénierie’, 98),\n",
      "‘Technologies’, 98),\n",
      "Ccomment\", 9@)]\n",
      "\n",
      "False)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "‘Feist plot (30, cumilative:\n",
      "\n",
      "pit. show()\n",
      "\n",
      "In [99]:\n",
      "\n",
      "samples\n",
      "\n",
      "In [100]: | #8igrams\n",
      "bigrm = list (nltk.bigrams(tokenized_word))\n",
      "words_2 = nltk.FreqDist (bigrm)\n",
      "words_2.plot(20, color='salmon’, title='Bigram Frequency’)\n",
      "\n",
      "Bigram Frequency\n",
      "\n",
      "‘de\n",
      "\n",
      "¢\n",
      "(er, technologies)\n",
      "\n",
      "Rate’ tis)\n",
      "\n",
      "(commen\n",
      "(starsikeCommentShaceo, Comments rite\n",
      "\n",
      "cs.\n",
      "(TaneationLikeCommentShare’ ‘Comment\n",
      "\n",
      "(Comments rite\n",
      "\n",
      "recommend\n",
      "\n",
      "(aingeniene’ et)\n",
      "\n",
      "(Prvee,ngénerie})\n",
      "(eomment Manama\n",
      "\n",
      "somples\n",
      "\n",
      "Out[100]: <AxesSubplot:titles{'center':'Bigram Frequency'}, xlabel='Samples', ylabel='Counts'>\n",
      "\n",
      "In [1es\n",
      "\n",
      "#Lexicon Normalization\n",
      "#performing stemming and Lemmatization\n",
      "\n",
      "from nltk.stem-wordnet import WordNetLemmatizer\n",
      "lem = WlordNetLemmatizer()\n",
      "\n",
      "from nitk.stem.porter import PorterStemmer\n",
      "stem = PorterStemmer()\n",
      "\n",
      "# Stemming\n",
      "from nitk.stem import PorterStemmer\n",
      "from nitk.tokenize import sent_tokenize, word_tokenize\n",
      "\n",
      "ps = PorterStemmer()\n",
      "\n",
      "Lemmatised_words=f1\n",
      "\n",
      "for w in filtereg_sent:\n",
      "lemmatised_words.append(ps-stem(w))\n",
      "\n",
      "print(“Filterea Sentence:\",filtered_sent)\n",
      "print(“lemmatised_ Sentence:\",lemmatised_words)|\n",
      "\n",
      "Business |»! Data\n",
      "understanding |_| understanding\n",
      "\n",
      "~~\n",
      "\n",
      "Data\n",
      "preparation\n",
      "\n",
      "Deployment tJ\n",
      "\n",
      "Modeling\n",
      "\n",
      "Evaluation\n",
      "\n",
      "\n",
      "ESPRIT Ecole\n",
      "\n",
      "commends ESPRIT\n",
      "\n",
      "\n",
      "In (188):\n",
      "\n",
      "In (189):\n",
      "\n",
      "In [199}:\n",
      "\n",
      "In [104}:\n",
      "\n",
      "out[104):\n",
      "\n",
      "‘#cLeaning data using french vocab\n",
      "from nitk.corpus import stopwords\n",
      "stop_words=set (stopuords.words(\"French\"))\n",
      "\n",
      "‘#cLeaning the vocab French and keep the key words\n",
      "filtered_sent-[]\n",
      "for w in tokenized_word:\n",
      "if w not in stop_words and w.isalpha():\n",
      "filtered_sent.append(w)\n",
      "\n",
      "+# Stemming(reLies on syntax of the word)\n",
      "from nitk.stem import PorterStenmer\n",
      "from nitk.tokenize import sent_tokenize, word_tokenize\n",
      "\n",
      "ps = PorterStenmer()\n",
      "‘stemmed_words-[]\n",
      "\n",
      "for w in filtered_sent:\n",
      "stenmed_words .eppend(ps.stem(w))\n",
      "\n",
      "nltk.dounload(‘wordnet')\n",
      "\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nitk_data] _C:\\Users\\Samar\\AppData\\Roaming\\nItk_data,\n",
      "[nitk_data] Package wordnet is already up-to-date\n",
      "\n",
      "True\n",
      "\n",
      "sord[@] for\n",
      "\n",
      "jordin\n",
      "\n",
      "\n",
      "In (21):\n",
      "\n",
      "In [22]:\n",
      "\n",
      "In [13]:\n",
      "\n",
      "out[13]:\n",
      "\n",
      "In [24]:\n",
      "\n",
      "In [15]:\n",
      "\n",
      "Split train and test set\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from nitk.tokenize import RegexpTokenizer\n",
      "import numpy as np\n",
      "\n",
      "‘tokenizer to renove unwanted elements from out data Like symbols and numbers\n",
      "‘token = RegexpTokenizer(r' [a-zA-20-9]+\")\n",
      "cv = CountVectorizer (lowercase=True, stop_word:\n",
      "\n",
      "‘english’ ,ngram_range = (1,1), tokenizer\n",
      "\n",
      "‘text_counts= cv.fit_transform(data[‘Likes'].apply(lambda x: np.str_(x)))\n",
      "‘text_counts\n",
      "\n",
      "<91xS sparse matrix of type '<cless ‘numpy.int64’>*\n",
      "with 91 stored elements in Compressed Sparse Row fornat>\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "Xtrain, Xtest, y_train, y_test = train_test_split(\n",
      "text_counts, data['ComentType'], test_size-0.3, random_state-1)\n",
      "\n",
      "from sklearn.naive_bayes import MultinonialNB\n",
      "#Import scikit-Learn metrics module for accuracy calculation\n",
      "\n",
      "from sklearn import metrics\n",
      "\n",
      "# Model Generation Using Multinomial Naive Bayes\n",
      "\n",
      "clf = MultinomiaiNe().fit(Xtrain, y_train)\n",
      "\n",
      "predicted= clf.predict(x_test)\n",
      "\n",
      "print(\"MultinonialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
      "\n",
      "MultinomialNB Accuracy: @.9285714285714286\n",
      "\n",
      "‘token. tokenize)\n",
      "\n",
      "Visualize per Date\n",
      "\n",
      "“2p wise 09\n",
      "O——O\n",
      "\n",
      "‘Se former autrement\n",
      "Total of Facebook reviewers comparing by the Count of Comments per Type\n",
      "previous start of the year\n",
      "Commentage #2 03\n",
      "Petey fet Comment. Conmen.as\n",
      "5 [Resse 3\n",
      "7 ineowe 3 :\n",
      "9 Positive 3 $s\n",
      "12 Positive 3 2\n",
      "15 postive 3 g 6\n",
      "pote 3 1: :\n",
      "18 Positive 3 cd\n",
      "25 |Postive 3 Goal: 11 (90.91%) 34\n",
      "5 |positve 3 B\n",
      "68 Positive 3 é,\n",
      "te posve 3\n",
      "2 (Neste 2\n",
      "Total Negative 35 5\n",
      "\n",
      "Positive Negative\n",
      "CommentType\n",
      "\n",
      "Average Likes Interaction per Nationality\n",
      "\n",
      "onality AVG Interaction Likes reacting Comments\n",
      "\n",
      "gner 0.00% 0 0\n",
      "' 58.33% 7 7\n",
      "\n",
      "\n",
      "oa Visualize per Date\n",
      "aie ame9\n",
      "\n",
      "‘Se former autrement\n",
      "\n",
      "Evolution of Esprit Facebook reviewers over the years Parcantageluireviswarelperuanden\n",
      "\n",
      "cy\n",
      "\n",
      "18 (23388)\n",
      "40\n",
      "Gender\n",
      "Male\n",
      "\n",
      "20 @ Female\n",
      "\n",
      "Total of Facebook reviewers\n",
      "\n",
      "\\ 59 (76.62%)\n",
      "\n",
      "°\n",
      "2017 2018 2019\n",
      "Year\n",
      "\n",
      "Percentage of reviewers per recommendations Total of Reviewers per nationalit\n",
      "\n",
      "Nationality @ Foreigner @Tunisian\n",
      "\n",
      "17 2.08%) — °\n",
      "\n",
      "Recommendation\n",
      "@ recommends\n",
      "\n",
      "@ doesn't recommend\n",
      "\n",
      "count of Reviewerskey\n",
      "\n",
      "\\ 60 (77:92%)\n",
      "\n",
      "\n",
      "© Linked {fi}.\n",
      "\n",
      "‘Sum of Number by City\n",
      "\n",
      "Select Country\n",
      "\n",
      "coisa\n",
      "a\n",
      "infra\n",
      "Helier pe f\n",
      "ge i\n",
      "\"antaorre la Vella _ ° /\n",
      "_ noe ‘Ankara 6\n",
      "runaer\n",
      "Netra\n",
      "\n",
      "Netherlands\n",
      "\n",
      "dividings\n",
      "\n",
      "C (Programming Language)\n",
      "\n",
      "Unified Modeling Language (UML)\n",
      "\n",
      "Project Management\n",
      "PHP\n",
      "\n",
      "x\n",
      "Facebook Skills Linkedin Facebook Where They Studied What they Do +\n",
      "\n",
      "Linked [f.\n",
      "\n",
      "Where Esprit Stuff followers studied accor to Linkedin\n",
      "\n",
      "Count of WhereTheyStudied 1\n",
      "x00\n",
      "\n",
      "230\n",
      "200\n",
      "10\n",
      "100\n",
      "+0\n",
      "° a me ee ee ee ee\n",
      "Ecole Ecole -—=ENSI-Ecole INSAT-—Higher_—=—snstut’=«=« PEN institut. —nstut’ «=» SUPCOM—Focult€des Universi ot institut Piereand_Unversté de\n",
      "Supérieure Nationale —_Nationale Institut Institute of Supérieurde Institut ‘Supérieur supérieur Sciences Tunis El ‘Supérieur Marie Curie La Manouba\n",
      "Privée d'ingénieurs des Sciences National des Technological Gestionde Préparatoire des Etudes d'informati ‘Mathémati ‘Manar des Arts University.\n",
      "Gingénieve de Tunis ge Sciences Studies of Tunis, «aux Technolog Physiques et Multimédia\n",
      "ae Tinfommatgn Applguées Nabe Keates en Natures e cela\n",
      "\n",
      "Technolog ‘et de Tech\n",
      "\n",
      "ingenieur .. Communica. Tunis Manouba(.\n",
      "\n",
      "WhatTheyDo\n",
      "\n",
      "Linked [J\n",
      "\n",
      "a\n",
      "\n",
      "Engineering\n",
      "\n",
      "veneer rveesy\n",
      "sonccve\n",
      "onc\n",
      "suneroseoor= (I\n",
      "—\n",
      "conmuniyarasecs sence\n",
      "viediasreconmuncaton I\n",
      "operands Mangement\n",
      "asad Ds\n",
      "\n",
      "Human Resources\n",
      "\n",
      "|\n",
      "a |\n",
      ";\n",
      "\n",
      "50 100 150 200 250 4300 350 400 450\n",
      "\n",
      "Number\n",
      "\n",
      "Kimball Model\n",
      "\n",
      "Datamart 1\n",
      "(NF)\n",
      "\n",
      "Cube\n",
      "en. Browser\n",
      "Reporting\n",
      "Layer\n",
      "°\n",
      "\n",
      "Datamart 1\n",
      "GNF) OLAP Cube\n",
      "\n",
      "Data Warehouse\n",
      "& (ON Star Schema)\n",
      "\n",
      "Datamart n\n",
      "(3NF)\n",
      "\n",
      "\n",
      "In [47]\n",
      "\n",
      "In [48]\n",
      "\n",
      "Names\n",
      "\n",
      "soup. select(\".5jgn65i0\")\n",
      "\n",
      "Names\n",
      "\n",
      "OutLa8]\n",
      "\n",
      "In [49]\n",
      "\n",
      "In [*]\n",
      "\n",
      "In [338]\n",
      "\n",
      "[<div classe\"duéu351b kurctbm 19j@che7 sjgnésia” ><div class=\"dudw351b 19j0dhe; \"><civ aria-descr\n",
      "ibedoy=\"jsc_c_7d jsc_c_7e jsc_c_7# jsc_e_7h jsc_c_7g\" aria-labelleaby=\"jsc_c_7c\" aria-posinset=\"1\" class=\"1zcicdu1\" role=\n",
      "\n",
      "ticle\"><div class=\"j3agx80 chudaoet” styl ‘aescxv 19J@che7 dudw351b mkhogb32\" hide\n",
      "\n",
      "div class=\"j83agx80 19Jadhe7 kaurcfbm\"><div class=\"rqdescxv 19j@che7 dudu351b hybvsuéc iodzgebd mSlcvass fbipléqg quvgtn77 k\n",
      "4urcfom nigdonod stjgntxs sbcfpzgs\" style=\"border-radius: max(Opx, min(8px, ((100vw - 4px) - 100%) * 9999)) / Spx;\"><div><ci\n",
      "V></divo<div><div><div></div><aiv><div class=\"pybrSéya datilna hvdrvrfc n8Sicfes btwodlt3 j83agxB0 118tlvén\"><div class=\"oi\n",
      "924de8 dodeu71z J83agxB0\"><span class=\"nc6B4n16\"><a aria-hidden=\"true” class=\"oajrixb2 g5ia77ul quex@S1F esrSnhéw e9989ues r\n",
      "Td6kgcz rqdescxv nhd2j8a9 nc6B4n16 p7hjInBo kvgnc6gS cxnmrSt oygrvhab hcukyx3x jb3wyJys rzubd8a qtécOcv9 aBnywdso i1a09sBh\n",
      "esuyzwur Flsipdof Lzcic4ul oodgrSid gprodwis” href=\"https: //uwy. facebook. com/rosaben78?__cft__[0]=AZXIn1Z09aS4G54VDyRaXnnJc8\n",
      "‘9r7SRDVSTEx4cKondqzRnVZLqYBpSSWNSDQDM2cOxU_FXexLr_AajcVESPpWBLAMZS@NHASQopyr}1Mgnv.gamp; \" role=\"Link\" tabin\n",
      "dexs\"-1\"><aiv class=\"q676}60p aypqoScg\"><object type=\"nested/pressable\"><a aria-label=\"Zeineb Be’ ajrixb2 gslasyio g\n",
      "Sia77ul mtka9kbi tlpLixtp gensuy8} popSayq2 goun2846 ccndeije s44p31tw mk2mcS#4 rtSo4zig ndej3031 agehan2a skéxxmp2 radescxv\n",
      "‘nhd2j829 qQuorilb mg4g7781 btworlt3 pnyh3mw p7hjlndo kvgncég5 cxmmrSt8 oygrvhab hcukyx3x tevbjepo hpfumrgz jb3vyiys rz4ubdB\n",
      "2 qtécOcv9 aBnyudso 19jadhe7 i1ac%s8h esuyzwir Flsipdof dudu35lb Izcicdul abiwirkh pSdawk71 oodgrSid” href=\"https: //mw.Face\n",
      "book .com/rosaben78?__c¥t__[0]=AZXIn1Z09aS4G54VDyRaXnn] c89r7/SnDYOTEx4cKondg2RnVZLqVSpSSuliSDQDm2cOxU_FXexLr_AajcVESPoHBIANIZSON\n",
      "HASQopyrjilgnvigaanp;_ta__=X3CK3CK2CP-R\" role=\"Link\" tabindex=\"@\"><div class=\"quorilb 19J0dhe7 p2ggbiyp dudw351b\"><svg ari\n",
      "a-hidden=\"true” class=\"pggbiyp\" data-visualcompletion=\"ignore-dynamic” role=\"none\" style=\"height: 40px; width: 40px;\"><mask\n",
      "ide\"jsc_c_7i\"><circle cx=\"20\" cy=\"20\" Fill=\"white” r=\"20\"></circle></mask><g mask=\"url(#}sc_c_7i)\"><image height=\"100%\" pres\n",
      "erveaspactratio=\"WiidWilid slice” style=\"height: 40px; width: 40ox;\" width=\"1008\" x=\"0\" xLink:href=\"https:://scontent.ftun10-\n",
      "\n",
      "ListNiane\n",
      "\n",
      "[pt.get_text() for pt in Names]\n",
      "\n",
      "import pandas as pa\n",
      "ListNames = pd.DataFrame.from dict (({\n",
      "“Data”: ListNane,\n",
      "\n",
      "})ordent=\"index' )\n",
      "Listnanes .transpose()\n",
      "\n",
      "ListNames .to_csv(r'C: \\Users\\Samar\\Desktop\\4B14\\Scrapping\\FB.csv', index-False,header=True, encoding = \"UTF-16\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In (11:\n",
      "\n",
      "In\n",
      "\n",
      "In\n",
      "\n",
      "In\n",
      "\n",
      "In\n",
      "\n",
      "(a6):\n",
      "\n",
      "a7:\n",
      "\n",
      "tas}:\n",
      "\n",
      "tas}:\n",
      "\n",
      "Data Fetching\n",
      "from selenium import webdriver\n",
      "\n",
      "# Creation of a new instance of Google Chrome\n",
      "browser = webdriver.Chrome(executable_patl\n",
      "\n",
      ": /Users/Samar/Desktop/4814/Scrapping/chromedriver .exe\" )\n",
      "from bs4 import Beautifulsoup\n",
      "\n",
      "# Navigating to esprit facebook page\n",
      "\n",
      "Link = ‘https: //mwu. facebook. com/esprit.tn/revieus/?ref=page_internal' # Profile Link which you want to scrape\n",
      "browser. get(1ink)\n",
      "\n",
      "browser.implicitly_wait(1)\n",
      "\n",
      "+# Rendering of the page\n",
      "Soup = BeautifulSoup(brouser.page_source, \"Lynl\")\n",
      "\n",
      "xv\n",
      "\n",
      "Date [=] Day Name [=) Age [=] Year [=] startotvear [=] month [=]\n",
      "Wednesday, August 23,2017 Wednesday 14782017, Sundoy onuary 1, 2017 2\n",
      "Thursdoy, September 7, 2017 Thursday 1459, 2017, Sunday, January 1, 2017 2\n",
      "\n",
      "Tuesdoy, March 28,2017 Twesday 41636 2017, Sundoy, January 1, 2017 1\n",
      "Thursday, une 22,2017 Thursdoy 415362017, Sundoy Janvory 1, 2017 2\n",
      "Tuesday, August 29,2017, Tuesday 14682017, Sundoy, January 1, 2017 2\n",
      "‘Sundoy, October 15,2017, Sunday 24212017, Sundoy, January 1, 2017 A\n",
      "‘Wedhnesdoy, September 13,2017 Wednesday 14532047, Sundoy Jonvary 1, 2017 2\n",
      "‘Friday, August 18,2017 Friday 41479, 2017, Sunday, January 1, 2017 2\n",
      "Wednesday July 12,2017 Wednesday 1516 2017 Sunday, Jonuory 1, 2017 1\n",
      "Wednesday, August 30,2017 Wednesday 1467/2017, Sunday lonuary 1, 2017 2\n",
      "‘Monday, September 4, 2017 Monday 41462 2017, Sunday, January 1, 2017 a\n",
      "‘Fridoy, September, 2017 Friday 414652017, Sundoy, January 1, 2017 1\n",
      "Friday, Morch 3,2017 Friday 41647 2017, Sundoy sanvary 1, 2017 2\n",
      "‘Sunday, July 16,2027  Sundey 41512, 2017, Sundoy January 1, 2017 2\n",
      "Soturdoy, September 2, 2017 saturday 414642017, Sundoy, January 1, 2017 1\n",
      "‘Tuesday, August 22, 2017 | Tuesday \"qa7s| 2027] sunday tonuary 2, 2027 7\n",
      "\n",
      "statof month [=]\n",
      "Tuesdoy August 22017\n",
      "‘Friday, September 1, 2017,\n",
      "Wednesday, Mare 12017,\n",
      "Thursdey, une 1, 2017\n",
      "Testy August 32017\n",
      "Sundoy, October, 2017\n",
      "‘Friday, September 1, 2017,\n",
      "Testy August 3, 2017\n",
      "‘Soturdy, say 2, 2017\n",
      "Tuesdoy August 22017\n",
      "‘Friday, September 1, 2017,\n",
      "‘ido, September 1, 2017,\n",
      "Wednesday, March 12017,\n",
      "‘Soturdy, uy 2, 2037\n",
      "‘ido, September 1, 2017,\n",
      "Teeeday, Auguet 1, 2017\n",
      "\n",
      "\n",
      "=| 0 =) 00206 comments [=]\n",
      "\n",
      "{rds bonne école privée dingénieri en tunisie\n",
      "Jess Sie 31.000 150 59 ond cinta 96a al\n",
      "\n",
      "hello | am kharroubi cover teacher aslana and parent responsibl:\n",
      "Le nouveau systéme du cours de soir est trés médlocre ls rvont p\n",
      "\n",
      "good graduates. Nice people\n",
      "\n",
      "‘When | registered online | did nt receive any emalls and thank ye!\n",
      "‘Mourad Zeal: Mamestou Mamestou, Chieka and Gaar from the b\n",
      "\n",
      "director ofthe house no politeness\n",
      "\n",
      "‘When I register on the site Ihave not found an interview datell3 ¢\n",
      "| would ike frst and foremost to express all my anger and distress\n",
      "\n",
      "‘Bonjour Je suis un licenciéen génie cvl promotion 2017 je veux s:\n",
      "\n",
      "‘Quiaete soumis dansia liste d'attente\n",
      "\n",
      "hello i wish you can give more importance to other engineering tr\n",
      "'Bnsr 3 tous je me nomme Romuald motcheho ete trouve que ce\n",
      "\n",
      "licence en sciences et technologies\n",
      "\n",
      "http://www facebook.com/Vente-des-machines.indusrielles-D\n",
      "\n",
      "experience excellence... qui parle.\n",
      "\n",
      "°\n",
      "\n",
      "Commenttype [~] Postkey [~] Datekey [~] Reviewerskey [=]\n",
      "\n",
      "Positwe 1 1 a\n",
      "Negatwe 2 2 2\n",
      "Positwe 3 3 3\n",
      "Negatwe 4 4 4\n",
      "Positwe 5 5 5\n",
      "Negative 6 6 6\n",
      "Negative 7 7 7\n",
      "Negative 5 5 @\n",
      "Positwe s s A\n",
      "Negative 10 10 30\n",
      "Positwe A a a\n",
      "Positwe 12 2 12\n",
      "Positwe 13 Fey 3\n",
      "Positwe 14 “4 7\n",
      "Positwe 15 a5 15\n",
      "Positwe 16 16 16\n",
      "Positwe 7 7 7\n",
      "\n",
      "Full Name hd\n",
      "\n",
      "Zeineb Be\n",
      "Houssine Kharroubi\n",
      "Youssef Mallat\n",
      "Imhamad Ayadi\n",
      "Oussema Sghaier\n",
      "Salwa Bourara\n",
      "Romuald Motcheho\n",
      "Maryem Tiili\n",
      "\n",
      "ail Aat all Zot\n",
      "\n",
      "Bii Lel D'khili\n",
      "Sami Hadded\n",
      "\n",
      "Houssem Eddine Lassoued\n",
      "\n",
      "Recommendation\n",
      "recommends\n",
      "recommends\n",
      "recommends\n",
      "recommends\n",
      "recommends\n",
      "recommends\n",
      "recommends\n",
      "recommends\n",
      "recommends\n",
      "recommends\n",
      "recommends\n",
      "\n",
      "recommends\n",
      "\n",
      "Gender |~] Nationality\n",
      "Female Tunisian\n",
      "Male Tunisian\n",
      "Male Tunisian\n",
      "Male Tunisian\n",
      "Female Tunisian\n",
      "Female Tunisian\n",
      "Female Tunisian\n",
      "Female Tunisian\n",
      "Female Tunisian\n",
      "Male Tunisian\n",
      "Male Tunisian\n",
      "Male Tunisian\n",
      "\n",
      "\n",
      "DON T\n",
      "STOP\n",
      "UNTIL\n",
      "YOURE\n",
      "PROUD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    " \n",
    "# get the path or directory\n",
    "folder_dir = \"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test/Images\"\n",
    "for images in os.listdir(folder_dir):\n",
    " \n",
    "    # check if the image end swith png or jpg or jpeg\n",
    "    if (images.endswith(\".png\") or images.endswith(\".jpg\")\n",
    "        or images.endswith(\".jpeg\")):\n",
    "        # display\n",
    "        text= pytesseract.image_to_string(PIL.Image.open(os.path.join(folder_dir, images)),config = myconfig)\n",
    "        print(text)\n",
    "        #print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "#extract text and write images in Temporary Image directory\n",
    "text = docx2txt.process(r\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test/Report_Samar_Jberi.docx\",r\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test/Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter_the_text_to_Search_hereby the Private Higher School of Engineering and Technologies \n",
      "C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test\\Images\n",
      "C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test\\Rapport_text_mining.docx\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test/Images\\\\image17.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-158-1741fd4e50dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mfolder_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test/Images\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocx2txt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\docx2txt\\docx2txt.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(docx, img_dir)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\".jpeg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\".png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\".bmp\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0mdst_fname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst_fname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdst_f\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m                     \u001b[0mdst_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzipf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test/Images\\\\image17.png'"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "import os\n",
    "import re\n",
    "import docx2txt\n",
    "# assign directory\n",
    "directory = 'C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test'\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "pattern = re.compile(input(\"Enter_the_text_to_Search_here\"))\n",
    "for filename in os.listdir(directory):\n",
    "    if not filename.startswith('~$'):\n",
    "        f = os.path.join(directory, filename)\n",
    "    \n",
    "    print(f)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        document = Document(f)\n",
    "        folder_dir = \"C:/Users/Samar/Desktop/5BI4/Stage PFE/essay/Test/Images\"\n",
    "        text = docx2txt.process(f,folder_dir)\n",
    "        \n",
    "        for para in document.paragraphs: \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "            if pattern.search(para.text):\n",
    "             \n",
    "                 print(\"Found the paragraph in this file\" ,filename)\n",
    "                 print(para.text)\n",
    "        for run in para.runs:\n",
    "                if para.text in run.text:\n",
    "                        run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "                        document.save(f) \n",
    "                break\n",
    "       \n",
    "        else:\n",
    "                 print(\"Did not find the paragraph :(\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
